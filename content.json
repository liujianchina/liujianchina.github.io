{"meta":{"title":"树先生的金融风控工程师博客","subtitle":"","description":"","author":"树先生","url":"https://blog.sofunnyai.com","root":"/"},"pages":[{"title":"about-关于我","date":"2018-03-21T08:41:44.000Z","updated":"2020-05-20T08:46:47.021Z","comments":true,"path":"about/index.html","permalink":"https://blog.sofunnyai.com/about/index.html","excerpt":"","text":""},{"title":"categories-博文分类","date":"2018-03-21T08:35:01.000Z","updated":"2020-05-20T08:46:34.397Z","comments":true,"path":"categories/index.html","permalink":"https://blog.sofunnyai.com/categories/index.html","excerpt":"","text":""},{"title":"tags-标签文章","date":"2018-03-21T08:41:37.000Z","updated":"2020-05-20T08:46:25.013Z","comments":true,"path":"tags/index.html","permalink":"https://blog.sofunnyai.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"风控策略相关的一切","slug":"风控策略相关的一切","date":"2020-04-15T06:37:22.000Z","updated":"2020-05-26T05:06:31.825Z","comments":true,"path":"article/strategy.html","link":"","permalink":"https://blog.sofunnyai.com/article/strategy.html","excerpt":"","text":"风控策略概要 什么是风控审批策略 其中多维度数据的策略规则包括： 风控审批策略的目的 风控审批策略的作用 风控审批策略的类别 风控的基本量化指标 确定目标变量 制定风控审批策略 策略预估 策略监控 策略回顾 风控策略分析师 日常工作内容 必备技能 核心作用 策略分析常见工作场景与对应分析方法 三方数据测评 举例—黑名单数据评测 准入策略的制定 年龄准入策略 地区准入策略 白名单策略 黑名单策略 规则阈值cutoff如何设定 背景 第一步：通过评分找到风险被低估的区间 第二步，评估拟拒绝人群的收益/风险比 通过率下降的策略调整 1.寻找通过率下降的时间点或时间段 2.判断策略节点主次要拒绝影响 3.从节点聚焦到节点规则层深度分析 4.具体规则分布分析 5.分析指导决策 逾期率上升的策略调整 第一步：确定存量还是新增客户导致逾期上升 第二步：多维度分析，找出最主要影响规则 第三步：制定策略调整方案 信用多头策略 1.什么是多头借贷 2.多头借贷数据的分析方法 3.多头借贷数据为何少用于模型 4.多头借贷数据在策略规则上的应用 评分的策略应用 评分卡模型的运用，主要是为了解决两大问题： 评分模型的cutoff 模型与策略的关系 模型是否可以替代所有的策略规则? 策略规则+评分模型 策略规则+模型规则 策略规则的粗放式管理 评分模型的常见三种盲区 建模数据集与实际贷款人之间存在偏差 模型数据集来自历史，与未来实际情况存在偏差 模型对于目标变量的界定与实际商业目标存在偏差 精简摘录自微信公众号：金科应用研院 风控策略概要# 什么是风控审批策略# 基于数据分析在申请阶段制定各式各样多维度的策略和规则; 其中多维度数据的策略规则包括：# 社交及短信维度规则 移动设备维度规则 外部数据源（如：征信报告、各种黑名单来源）规则 多维度评分卡规则 行为数据(设备信息、注册时间、登陆时间)规则 风控审批策略的目的# 在贷前审批减少风险事件的发生的各种可能性，挽回风险事件时造成的损失。较大的程度上筛选过滤高风险客户，保留低风险客户予以营销。针对客群分级实行个性化的审批流程，提高审批效率。 风控审批策略的作用# 在保证业务量的同时降低业务坏账率、控制逾期风险，最终实现公司盈利。 风控审批策略的类别# 多维度数据分析呈现了借款人的用户画像，制定多维度完善的审批策略规则，具体策略规则包含： 1）经济能力维度(月收入、工资流水等信息) 2）app信息维度(贷款APP安装个数、短信命中高风险关键词) 3）基本信息维度(年龄、性别、工龄等信息) 4）信用历史(征信贷款信息、还款记录) 5）行为表现(活动轨迹、登陆时间、注册时间等信息) 风控的基本量化指标# FPDx：首期逾期，x对应天数 CPDx：当前逾期，x对应天数 逾期时间的长短来定义逾期的等级，C代表正常资产。M3-M6属于不良，M6+属于坏账。 迁移率、vintage账龄分析、滚动率见上一篇博客，这里：https://blog.sofunnyai.com/article/vintage_rollrate_fpd.html 确定目标变量# 根据催回率及迁徙率确定好坏客户（不过一般还是用滚动率比较多） 由上表可以看出，M2以上的迁徙率将近90%，所以确定当前逾期31天以上为区分好坏客户的标准，及后续分析的目标变量。 制定风控审批策略# 如以城市等级为例，城市等级与逾期的关系：城市等级越低，其对应的逾期率越高。 策略预估# 预估策略上线对生产运营阶段的影响，基于进件量、放款量、通过率的影响。 策略监控# 策略上线后，监控此策略的占比与预计的占比是否发生严重偏差，且在正常运行阶段是否全部执行。 策略回顾# 对上线后的策略，在一定时间后。对于有表现的数据进行策略回顾，看策略调整后的进件量、通过率及贷后表现。若是想及时的查看策略上线后的贷后表现可以针对FPD指标分不同的天数去观测，FPD4，FPD10，FPD30等。 若策略是调宽或者是放松时，可以针对性回顾下豁免出来的客户的进件情况、通过率及贷后表现。 若策略是调严或者收紧时，可以针对性回顾拒绝阈值边缘维度的贷后表现及拟定拒绝的客户数。 风控策略分析师# 风控策略分析师是完成上述P1部分所有分析，构架风控策略的人员。 日常工作内容# 贷前、贷中及贷后各环节的风险策略与流程，制订各项策略规则，具体包括准入、授信、定价、用信、还款、调额等信贷流程各阶段的策略规则 通过对各类风险指标与报表的分析，关注各类资产和客群的风险变动，对公司全渠道风险政策与策略进行跟踪评价，并及时优化调整相应的风险政策与策略 必备技能# 结合内外数据，通过统计分析方法，对不同风险点制定出不同类型的风险规则 完成整个贷前、贷中和贷后的风险规则架构，实现自动化风控 可以实现策略规则优化，不限于A、D类调优方法 规则的部署与监控预警 临时指标调整的项目经验 核心作用# 实现具体规则和流程的设计、开发、部署、监控与优化 策略分析常见工作场景与对应分析方法# 三方数据测评# 案例：现有1000个样本数据，分别测试2家黑名单，2家欺诈名单与2家多头，如何选择合适的第三方数据源？ 首先要专业科普选择第三方数据源重要考察的5大指标计算公式（以黑名单为例）： 查得率(Search rate)=查得数/样本量 覆盖率(Cover rate)=查得命中黑名单数/样本中命中黑名单量 误拒率(Error reject rate)=查得命中黑名单数/样本中通过且为Good量 有效差异率(Effective difference rate)=查得命中黑名单数/样本中通过且Bad量 无效差异率(Invalid difference rate)=查得命中黑名单数/样本中其他拒绝量 其中SR、CR、EDR指标越高越好，ERR越低越好，IDR与EDR结合起来观察，如果IDR和EDR都较高，反应的一种情况是数据源定义黑名单是广撒网式，黑名单质量相对不够精准。 其中前三个指标是重点考察，如果想更全面的测试第三方数据源，后面两个差异率指标也可以加入考核标准。 举例—黑名单数据评测# 1000个测试样本数据中，审批结果字段表示样本通过和拒绝，其中通过样本中有未逾期和发生逾期的客户样本，拒绝样本中有通过黑名单库拒绝客户，也有其他原因产生拒绝。比如，数据源1（黑名单）代表一家提供黑名单数据的数据供应商A，数据源2（黑名单）代表另一家提供黑名单数据的数据供应商B，以此类推。 对1000条测试数据返回结果进行整理可以总结出如上数据概要，对比看到数据源1的返回结果如下： 按照文章开始介绍的指标分析方法，对比数据源1和数据源2的测试结果可以得出如下结论： 数据供应商1的查得率、覆盖率高于数据供应商2大约5%、4%； 数据供应商1的误拒率低于数据供应商2大约0.3%； 数据供应商1的有效差异率低于数据供应商2大约8%，无效差异率低于数据供应商2大约7%； 依据五大指标分析标准，SR、CR、EDR指标越高越好，ERR越低越好，IDR与EDR结合起来观察，如果IDR和EDR都较高，反应的一种情况是数据源定义黑名单是广撒网式，黑名单质量相对不够精准！ 最终分析结论： 数据供应商2虽然覆盖的黑名单比数据供应商1的更广，但其不如数据供应商1精准，更偏向选择数据供应商1的黑名单数据。 准入策略的制定# 风控准入策略作为金融借贷机构评估一个借款人是否有机会获得授信的第一道门槛，是保卫金融机构的第一道护卫。 风控准入策略属于贷前风控策略体系的一部分，贷前风控策略包括基础认证、准入策略，贷前反欺诈策略，黑名单策略，特殊名单策略及信用风险策略。风控准入策略中的规则更多是由产品政策性规则构成。 针对不同信贷场景采取更适应业务的准入规则，设定科学的准入策略，对于风险的防范与降维有十分重要意义。合理的风险准入策略，也能对信贷业务的走向与风险倾向产生直接影响，进一步影响金融机构的最终盈亏。 为什么要设计风控准入策略 风控准入策略的规则属性全部为强拒绝规则（硬规则），借款人一旦不满足一条准入规则金融贷款机构都不会给予贷款的授信与发放；同时，风控准入规则不需要经过复杂的规则衍生，通常可以简单有效的判决借款人是否有资格进入之后的风控流程；最后，风控准入规则的策略理念是验证借款人依法合规未被政策限制。 风控准入策略模块 风控基础认证模块: 基础认证模块主要作用是验证借款此人是本人，也是以风控规则形式出现，规则大多为公允共认的规则。比如身份证信息验证，人脸信息验证、银行卡四要素验证、运营商三要素验证等。 在验证完借款人基础信息后，风控贷前流程才会进入准入策略模块。 准入策略模块主要分为年龄准入、地区准入、行业准入及其他。这些准入规则的根本设定原则是基于监管和金融机构产品政策性导向。 年龄准入策略# 对于年龄准入而言，中国银行业监督管理委员会令《个人贷款管理暂行办法》中指出个人贷款申请应具备以下条件： （一）借款人为具有完全民事行为能力的中华人民共和国公民或符合国家有关规定的境外自然人； （二）贷款用途明确合法； （三）贷款申请数额、期限和币种合理； （四）借款人具备还款意愿和还款能力； （五）借款人信用状况良好，无重大不良信用记录； （六）贷款人要求的其他条件。 其中借款人具有完全民事行为能力的中华人民共和国公民年龄范围在18-60岁。所以合规的金融机构信贷产品的借款人年龄准入策略中，年龄规则的设定是：年龄&gt;X &amp; 年龄&lt;X ，X属于18-60。有些贷款产品，则是根据贷款人的性别不同来限制年龄的。比如对于女性申请人的年龄限制是22周岁以上，而男性申请人的年龄限制为20周岁。 地区准入策略# 一般金融机构会按照风险热力地图将一些重灾风险区进行隔离或者进行“象征性”政策贷款发放。 地区准入规则的初始设定一般是风险集中度比较高、社会稳定性比较弱、地区经济GDP比较低，亦或是难催收的地区，比如新疆、东北等个别地域。 在之后随着信贷业务的开展，也会根据贷款回收率对地区准入规则进行一些策略调整，比如一些地区的贷款回收率长期观测较低，金融机构企业内部信贷战略调整后，可以将这些地区加入限制性地区里。 上图事例1中展示的是M1阶段的回收率热力分布地图，可以发现灰色区域的M1贷款回收率低于60%。如果需要进行地区准入策略的调整，还可以将M1回收率0-60%的区间划分的更细，比如0-15%的M1回收率，也可以将省维度拆成市级维度进行限制。 此处需要提醒，一旦加入到地区准入规则后的地域在之后将无法进行信贷业务，同时也会失去观测业务数据，所以此类的策略调整要谨慎设计。 地区准入策略的常用规则如户籍地址 in（x,x,x），单位地址 in （x,x,x）,家庭地址 in （x,x,x）等。 行业准入策略 行业准入策略的基础原则是对一些行业工作不稳定或无业的借款人禁止提供信贷业务。如禁止： 金融属性行业如投资、担保、理财、典当；政策性敏感娱乐行业如KTV、按摩院、会所等；无业和自由职业、学生、媒体工作者，检察院。 白名单策略# 白名以下两种业务场景： A.在存在自有存量数据的前提下，金融机构想开展信贷业务，前期需要通过白名单控制入口，此类场景多存在于业务初期，或者是内部员工贷的业务场景。(风控模型不完善的条件下，先把业务开展起来。同时，在这个展业的过程中，可以逐渐组建适合金融机构业务的风控策略和模型。) B.在业务开展中期，需要部分进件客户走特殊贷前审批流程，满足特殊审批的要求，此类场景多存在于较大的金融公司。(有着较好的信用、较好的资产亦或是较好的“背景”，通过一些特殊审批流程进行贷款的审核，最终满足“VIP”的借贷需求。) 综合来讲，白名单可以定义为，通过金融机构内部现有数据判断的“好客户”，或者经过一系列规则挖掘分析得出的“好客户”，由他们组成的借贷优质名单。 如何筛选出白名单 联合建模： 金融机构在有存量数据的前提下，自有数据是不缺乏X特征变量，主要缺乏相应业务场景有表现特征的目标Y变量。在这个时候可以通过引进一些外部机构进行联合建模，用以补充一些Y变量。 通过与外部机构联合建模得出评分，不论是将其用于内部客户分层，还是将评分分数直接做规则，都对筛选白名单有很好的帮助。 内部数据探索： 我们在筛选白名单的时候，除了通过联合建模弥补相应业务场景下目标变量的缺失，还可以通过内部数据探索，寻找分析一些对逾期违约表现相关性较强的一些特征规则，逐渐设定出白名单规则。这里面分为两种规则设定方式。 第一种是寻找与新开展业务相似模式和场景的已有产品，参照已有产品的风控策略规则对新业务场景数据进行比对分析，参照已有产品的策略规则制定出新业务场景的风控白名单规则。 另外一种方式是在更“艰苦”的环境下，没有任何可对比参照的已有产品，这个时候设定的白名单规则相对更严谨，同时对风控策略工作者的业务经验要求更高，可以认为是一种专家经验规则。 引入外部数据匹配： 工作单位、学历、社保缴费单位、公积金缴费单位、缴费基数等信息去筛选优质客户。 白名单的作用： 控制放量节奏：初期的时候用于控制节奏，整体调控。 降低风险 提高过审率 协助调整贷前策略：白名单筛选的过程就是贷前策略的一部分。 黑名单策略# 黑名单：性质极其恶劣的坏客户。无论是其还款能力，还款意愿，借款目的等都不能满足正常客户的标准。 有自建和外部引用两种。对于业务初期的金融机构通常调用三方数据接口查询行内黑名单客户，同时在自家展业过程中，通过贷后管理逐渐补充、完善自家黑名单库。 黑名单的使用：# 一般来说金融机构一旦触碰到黑名单规则，金融机构通常会全部拒绝。（全部拒绝黑名单前，会随机放过5%或者10%的触碰黑名单的客户，去测试黑名单数据有“多黑”，测试该黑名单客群是否适用于该机构。） 导流助贷机构可能会选择性放入一部分客群，结合客户评分，多头等数据综合判断，或者随机放过。（反正不是他兜底） 敞口测算# 假设一个场景：如果有一万块钱，借款一年，不考虑其他，综合年化36%的信贷产品，因为一个黑名单客户导致本金全部损失，那么实际上需要大约3个好客户才能弥补1个坏客户的损失。 如果我们加上资金的运营成本，人力成本，引流成本，实际成本等。 假设需要的综合年化是15%，那么实际上 ，也就是5个好客户才能覆盖一个坏客户的本金损失，同时还需要覆盖上述的各种成本 ，也就是说金融机构大约要用6个完全的好人才能替代一个完整的坏人。 如果是3期、6期产品，同时也包含资金占有率问题，实际上需要的用更多的好人去覆盖坏客户带来的损失。 假设5000 6期 36%，每个好人收益是5000×0.36/2 = 900，不算其他成本也需要5.5个人才能cover一个欺诈。（此时坏账率：1/6.5=15.4%） 实际同上假设人力、引流、IT，使得实际年华15%，每个好人收益是5000×0.15/2 = 375，需要13.3人才能cover。（坏账率1/14.3=6.99%） PS：助贷导流客户平均成本一般不会超过5毛钱，金融机构开展信贷业务所需风控数据成本也不会超过10元。 所以黑名单很重要，坏账很难搞。。。 自建黑名单# 黑名单一般的自建维度有参照回款表现、渠道、利率、各种公布失信类客户以及通过爬虫获得的一系列坏客户，黑名单的设定不一定仅限客户本身，也可以拓展为身份证、手机号、邮箱、银行卡、ip地址等，都可作为自建黑名单的参考维度。 和通讯记录、电话簿、二度通讯记录等联系起来。 自建黑名单命中率通常不会太高（相同客户再次注册的概率较低），且自建黑名单库需要长期的业务积累过程，因此金融信贷机构常常需要借助三方金融科技公司的黑名单库服务（特指三方数据供应商商以及其他金融信贷机构）。 大量p2p以及小贷机构接入百行征信，但我想要说明的是：滞后性和成本的增加使得黑名单需要更多的共享，只有共享才能更全面了解我们金融机构所接触的客群。 规则阈值cutoff如何设定# 风险策略拒绝线的设定，背后有严谨的分析逻辑，本文就以评分分数区间和年龄规则为例，为大家讲解审批策略拒绝线的内在分析方法。 背景# 评分模型，尤其是主流基于线性Logistic算法的评分模型，对于一些边际评分区间的风险，其实常常无法精准的预估到，势必会造成一些区间风险被低估的现象。如果不通过一些规则维度的拒绝补充，容易因为模型风险发生不必要的利益损失。 假设我们已经对评分模型分数分为T1-T5组，T1风险最低T5风险最大。年龄规则也使用单变量树模型初步分为5组区间。我们希望结合评分分数找到年龄规则这个核心策略维度的合理拒绝线。 第一步：通过评分找到风险被低估的区间# 本例中，首先将年龄与评分卡进行交叉矩阵分析，观测不同交叉区间里的用户违约概率。 一般策略规则多数组之间的趋势线是紧密相近的。从图示数据走线可以发现，年龄组[35,47)和[47,53)这两个年龄组的违约概率走线脱离了其他分组，尤其是年龄组[35,47)，其走线脱离其他“群体”过多。通过分析初步定位年龄组[35,47)和[47,53)可以是待确定的规则拒绝线。 第二步，评估拟拒绝人群的收益/风险比# 虽然经过评分与年龄的交叉对比，发现年龄规则的两个待确定高风险拒绝区间。但是实际拒绝线的划分要结合年龄分组区间人群的实际收益与风险进一步考虑。如果高风险的人群可以带来高收益，对于策略来讲也是可以接受的。 将年龄分组区间按照上图示例2横轴所示指标进行统计，假设年龄分组[35,47)的收益/风险大于[47,53)且为正，即表明虽然[35,47)年龄分组的人群违约率最高，但其收益同样也是最大。反而[47,53)年龄区间的人群为公司带来负收益。 本着收益覆盖风险的商业理念，此时对于年龄这一维度的策略最佳拒绝线，应该划分在[47,53)这一分组区间。 通过率下降的策略调整# 审批通过率和不良率是一对权衡指标，在新业务上线初期，维持一个较低的通过率可以保证最好的客群进去。随着业务规模做大和风控样本积累，此时需要在风险容忍度可接受范围内提升通过率，以保持收益的最大化。 如果某一天风控通过率忽然降低，这种情况下策略分析人员应该如何应对？ 1.寻找通过率下降的时间点或时间段# 在风控策略稳定之后，审批通过率一般稳定在某一小范围内波动，当监控每日通过率指标时发现，T-1、T-2时点的通过率明显下降，我们应该先通过监控报表迅速定位到具体时间点或时间段。 2019.6.23和6.24授信通过率下降。 Tip：上图示例通过率下降到6.9%、7.0%可以直接用肉眼分辨数据，但实际业务一般建议以通过率趋势图和PSI指标监控通过率下降。 2.判断策略节点主次要拒绝影响# 发现通过率下降的时间点或时间段之后，下一步先聚焦到策略节点。本文为FALers举例两个策略节点A（准入）和B（规则）。以6月23日为时间节点划分，对比数据分析，寻找拒绝率的波动差。 上图示例2中波动差按照B段A节点拒绝率-A段A节点拒绝率计算出来，以此类推。此时计算波动差仍然可以考虑加入PSI=(B-A)*LN(B/A)测算波动差,A节点的PSI为0.77%，B节点的PSI为0.01%。 按照波动差确定通过率的下降主要因为A节点的拒绝率上升引起，从而将通过率下降的影响因素从策略A和B两个节点问题进一步聚焦到A节点上。 3.从节点聚焦到节点规则层深度分析# 完成节点的聚焦分析，定位到引起通过率下降的主要原因节点A，接下来需要进一步分析节点A内包含的所有规则拒绝情况。 与节点聚焦分析一致，寻找引起拒绝率上升的主次要拒绝规则。在规则层确定主次要影响因子时，分析方法不仅结合数据同时也参考业务场景。 从上图示例4可以发现，按照波动差分析得出年龄准入拒绝和X3_准入拒绝是主要引起通过率下降的规则。 4.具体规则分布分析# 从步骤3确定出年龄准入拒绝是第一位引起通过率下降的规则后，第四步就从规则层聚焦到具体策略规则的分布上。 通过分析具体策略规则分布的波动差定位具体策略规则的某一分布，找出引起通过率下降的主要策略分布。 从上图示例6可以发现，年龄准入拒绝这一策略规则中，18-25岁的分布拒绝率在时间A段和时间B段的波动差最大，这个年龄分布的拒绝率上升可能是引起整个审批通过率下降的主要规则分布。 造成以上18-25岁年龄分布拒绝增加的原因，很常见的一种是进件客群发生了变化，针对客群发生突然变化的情况，如何将分析结果指导决策执行，是策略分析最后且最重要的一步。 5.分析指导决策# 仍以上述案例为例，通过一系列聚焦分析发现，18-25岁的进件客群变化是引起整体通过率下降的核心因素。实际业务场景中，并不会因为此时通过率突降就进行策略规则的调整，更多的是通过聚焦分析后，结果进一步细分两个参照要素：进件渠道的进件量分布和最大进件渠道的年龄准入拒绝分布。 5.1.进件渠道分布分析 既然是客群的变化引起了整体审批通过率的下降，从进件的所有渠道数据中进行分布排序，定位到渠道进件量A段和B段都最大的一个进件渠道C。 5.2.最大进件渠道的年龄准入拒绝分布 通过进件渠道进件量分析，从众多进件渠道中定位到最大进件渠道C。此时分析主要拒绝规则-年龄准入拒绝的渠道C的分布情况，是否满足条件：B段与A段年龄18-25岁的波动变高。 从上图示例8中分析发现，渠道C年龄在18-25岁的客群进件量在B段比A段上升明显，即从渠道进件前段业务确定出引起通过率降低的主要进件渠道C。至此，可以进行策略分析决策建议。 5.3.决策建议 将策略分析结果应用于前段业务指导和决策，提醒前端业务人员在渠道C可以适当缩紧18-25岁客群的进件需求，以此共同维护金融公司整体风控通过率，这才是风控策略分析工作者最终的使命和义务。 逾期率上升的策略调整# 当逾期升高时，如何进行策略调优。 真实案例背景（数据已脱敏）： 通过PQR监控报表发现，某XX贷产品的MOB报表自2019年5月开始，后续放款月资产逾期呈上升趋势，既DPD30+逐月上升，且上升速度逐步增快（MOB期数逐渐缩短）。在2019年11月放款的客户里，MOB=4的DPD30+等于2.49%。如下图1所示。 通过将MOB制作成Vintage报表，可以观测到某XX贷产品的风险自2019年5月到11月的DPD30+平均值位于6%的水平，如下图2所示。 往期DPD30+表现出的风险水平逐月快速上升现象，意味着如果不做相应的策略调整，之后的放款月风险将会更快的暴露。 针对此时逾期快速上升的背景下，如何分析策略，进行策略调整呢？ 策略分析方法 第一步：确定存量还是新增客户导致逾期上升# 信贷业务每个月发生授信和放款的客户可以分成新增客户和存量客户。从上图示例2中Vintage报表展现的数据，反映出资产逾期呈上升趋势。那我们首先需要将2019年5月到2019年11月（可观测到DPD30+）的Vintage分成新增客户的Vintage1和存量客户的Vintage2，如下图3。 从上图3的Vintage1（新客户）和Vintage2（存量客户）标注的红色椭圆框可以观测到，新客户的DPD30+平均处于6%，存量客户的DPD30+平均处于5%。与图示1对比可以分析出，导致资产逾期上升的主要原因是新增客户资产变差的影响。 至此阶段的分析结论：我们可以确定出需要调整的策略规则是贷前规则。 解释如下：往期放款月中，新客户是由贷前规则通过后，给予授信并放款的，存量客户的复借是由贷中规则决定。通过Vintage1和2的分析比对，引起资产逾期上升的主要原因是新客户的逾期上升。 第二步：多维度分析，找出最主要影响规则# 通过第一步的分析确定出核心要调整的是贷前策略后，我们接下来要通过分析不同的规则变量，找出对目标变量（DPD30+）影响最大的维度变量。 这里提供分析主要影响变量的两个思路，具体实践过程就不在这里多讲，文末推荐阅读有链接。 思路一：自上而下地按照A类策略调优方法，从贷前策略节点到节点里的规则集，再细分到具体规则，逐步分析出影响较大的规则变量（文末推荐阅读给出具体分析的往期文链接） 思路二：自下而上地将所有规则变量与目标变量拟合分析，通过IV的降序排序，找出影响较大的规则变量。 分析得出，城市等级是影响逾期目标上升的主要变量。通过分析2019年5月至11月的城市等级Vintage曲线，可以发现“其他城市”较“一线城市”、“二线城市”、“三线城市“对逾期的影响较大，如下图。 第三步：制定策略调整方案# 通过上述数据分析，发现贷前风控规则里的“城市等级”规则”其他城市“是导致逾期升高的主要原因。此时容易出现的一个错误决策是拒绝“其他城市”的进件。 原因很简单：这种决策会导致大量的申请被拒绝，对通过率的影响比较大。 最优的策略调整方案思路是：从“坏客户”中挑选出“最坏”的一批客户，且这批客户的占比较少，然后加以拒绝。 按照上述思路，我们可以制定出如下的策略优化方案： 1、进一步分析“其他城市”里，哪一些的城市逾期较高； 2、挑选部分逾期较高的城市做贷前准入规则。 以上，就是逾期升高情况下，策略调优的分析方法。 信用多头策略# 金融风险管理中，对于一个借款人还款能力的评估十分重视。如果一个人的资产负债比过大，一旦发生资不抵债的现象，金融机构继续对其发放贷款发生违约的风险是极大的。 在体现借款人甚至借款企业还款能力的众多指标中，多头借贷是一项核心指标。 1.什么是多头借贷# 多头借贷是指单个借款人向2家或2家以上的金融机构提出借贷需求的行为。多头借贷数据一般至少会粗分成银行类多头借贷、非银类多头借贷。按时间跨度可以分为近7天、近15天、近1个月、近3个月、近6个月、近12个月。 多头借贷除了会统计申请次数，还会统计申请机构数、申请最大间隔天数、申请最小间隔天数、申请记录月份、平均每月申请次数(有申请月份平均)、最大月申请次数、最小月申请次数等。 由于单个用户的偿还能力是有限的，向多方借贷必然蕴含着较高的风险。一般来说，当借贷人出现了多头借贷的情况，说明该借贷人资金出现了较大困难，有理由怀疑其还款能力。 2.多头借贷数据的分析方法# 由于多头借贷可以比较有效的反应借款人的还款能力，所以在对借款人信用风险、欺诈风险评估上，基本都有使用多头借贷数据。 多头借贷作为一个衡量借款人的维度特征，可以结合一些逾期指标进行分析。 上图示例1中，对近7天非银机构申请机构平台数进行分析，对申请不同平台数的客户，分别统计客群的分布占比、FPD30%、FPD30-DPD90+%、通过单量、FPD30单量、DPD90+单量以及DPD90+%。通过统计后的数据，分析近7天申请N平台数的客户，其不同逾期指标的变化趋势，如上图示例1中FPD30%的增幅，进一步用于寻找策略切点或者豁免客群的回顾分析。 3.多头借贷数据为何少用于模型# 多头借贷少出现在模型变量中，主要有两个方面原因。 第一，多头借贷数据往往被策略同事应用于规则中。 数据建模的目的是从金融弱变量中通过特征工程方法，提炼出有效区分变量，构建评分模型。所以对于多头借贷数据，既然已经运用在策略规则中，实在没必要加入到模型变量。如果读者朋友们看到提交的评分模型报告中有多头借贷变量，那么建模的同事要么没有事先了解已上线运行的策略规则集，要么就是为了模型表现指标（如KS、AR、AUC）好看强行使用。 第二，多头借贷数据往往覆盖度不全。 多头借贷虽然是一个与风险强关联的维度，但其查得率一直被人所诟病。 举一个例子，借款人一个月内在多家机构贷款，作为一个特征，很有可能出现某个人虽然频繁贷款，但并没有被多头供应商捕捉到。一旦这个特征作为模型变量，那么这个变量的噪声就很大了。反而如果做成反欺诈策略，就不需要担心噪声问题，直接选取拒绝线进行截断，最大的影响，也就是没有拒绝掉足够多的用户，而这个影响我们还可以用噪声较小的模型进行弥补。 4.多头借贷数据在策略规则上的应用# 多头借贷在策略上一般作为一条策略规则，一个拒绝维度参与到整个风控流程中。不同机构，不同信贷产品，不同场景，对于多头借贷的拒绝线划分都是不一样的。如何找到当下最适合的多头借贷拒绝线，对于风控策略分析人员，是风控工作的核心任务。 仍以上图示例1为例，假设当前对于7天多平台数规则的拒绝线划分在6，即如果7天多平台数&gt;=7则拒绝。如果我们现在希望通过7天多平台数规则豁免一部分客群提升整体通过率，此时的拒绝线cutoff应该划分在哪里呢？ 如果不是应对紧急调整通过率的情况，我们可以事先豁免7天多平台数7-10的客户，作为测试样本，用以产生7-10客群通过单量的分布，之后将拒绝线调回6。既可以生成如下统计分析表： 上图示例2中的桔色部分都是通过分析预测出来，比如通过上图示例1中不同多平台数FPD30%的平均增幅0.7%，预测出7-10的FPD30%。 预估计算公式8FPD30%=7FPD30%+0.7%。进一步计算出FPD30量、DPD90量等其他指标。 提醒读者朋友们，因为我们对于资产风险管控最关心的逾期指标还是不良率，所以我们通过FPD30-DPD90+%的迁徙率预测出不同7天多平台数的DPD90+%。对于7-10的FPD30-DPD90+%预估，可以采用MAX(0-6的FPD30-DPD90+%)的预估方法。 在这之后，我们对于不同7天多平台数测算出拒绝线Cutoff的FPD%和DPD%，如下图所示： 对比示例图1和图3的Cutoff_DPD%可以发现，规则拒绝线设定在&gt;=7时DPD%=3.0%，设定在&gt;=8时DPD%=3.0%，设定在&gt;=9时DPD%=3.3%。规则拒绝线设定在&gt;=8的DPD%并没有增加。此时可以尝试建议将7天多平台数的拒绝线调整到7。 当然，这种策略分析方法仍有一些纰漏，比如此方法需要有测试样本进行观测，无法满足快速调整通过率的需求；7天多平台数的FPD30%的增幅实际情况并非线性增长，有经验的策略分析师知道，FPD30%一定会在某一个节点指数级增长。 但正是因为策略分析师通过不断地按照上述方法进行样本测试对照，根据实际情况回顾分析结果，才能不断的积累策略调整经验，才会对规则分布具有一定敏感性。 评分的策略应用# 评分卡模型的运用，主要是为了解决两大问题：# 1、线上借贷业务量逐渐增加的情景下，策略规则已经无法满足更细的切分需求； 2、对于策略无法有效识别的大量灰色客群，需要使用评分卡进行风险判断； 现如今业界使用评分卡模型，更多的是为了解决第二个问题。 从金融机构自身业务发展历程来看，评分卡模型介入风险管理流程常常取决于两个重要的时机： 1、业务快速发展阶段 在金融机构业务发展的早期阶段，因为业务量小、样本少、风险控制严格等一些主客观原因，使用风控策略规则足以开展业务，所以在业务发展早期评分模型基本没有任何用武之地。 但随着信贷产品的测试期结束，金融机构要加快业务发展，此时不论是大量的客群样本、逾期表现的积累，还是风险控制的政策放松，都因为风险策略无法精准细分的局限性，而需要评分模型的介入，评分卡的应用场景更适用于人工分流。 此阶段的评分模型，常常表现不稳定，比如KS波动较大，Lift下降较快，PSI时常过0.1。此阶段评分模型的优化更多在于分析波动原因，快速重新开发迭代。 2、业务发展稳定阶段 一旦金融机构度过了新产品的早期和发展期，此时产品市场表现已经趋向稳定，反应在客群分析上，表现出稳定层级的客户画像，此阶段是评分模型介入2.0阶段。 在这个阶段评分模型会在风控流程节点上进行一些调整，比如申请卡模型会进一步的前置，担当部分客群豁免的功能。同时，此时评分模型介入2.0阶段也会降低一些外部征信数据调用成本，控制因三方数据有误而引起的误杀。 此阶段的评分模型，表现较为稳定，KS、Lift、PSI等指标波动较小，对于评分卡的迭代开发需求降低，评分卡的应用更加与业务需求、金融政策以及企业发展战略相关，在保证评分模型稳定性及相对精准度的前提下，使用模型调整系数进行全局模型的调整是此阶段的主要优化办法。 评分模型的cutoff# 评分卡分数转换出来，在不同业务发展阶段如何合理的制定评分的cutoff，是评分应用重要的一步。 一般将评分等分后，会有两种方式对评分进行cutoff：一种是参照KS和Cum % bad rate,另一种根据等分后的累计净收益。 第一种参照Max KS和累积bad rate理论上是可以尽可能的将坏客户剔除，对好客群进行授信，但无法根据业务发展需要保证收益最大化。参照不同业务发展阶段的需求，根据评分对收益损失预估，最终确定评分cutoff，我认为这才是精细化的评分应用策略。 第二种制定评分的cutoff，需要联动分析以下图示的一些指标 通过逆向累计净收入指标的分析，结合当下风控政策，综合评定评分的cutoff，将之应用在风控策略上，这样才是更接近业务的评分cutoff。 模型与策略的关系# 评分模型在金融信贷风控领域的应用非常广泛，模型的开发、监控也趋于标准化。 评分模型可以为每一位观测对象打出一个评分分数，理论上实现风险与定价的绝对对等，实现个体差异化的风险管理，在这点上，风险策略规则是远不可及的。 模型是否可以替代所有的策略规则?# 此时就有了风险策略与模型之间的争议：模型是否可以替代所有的策略规则？（排除政策准入规则） 想要回答上述的争议，首先需要了解目前策略规则与模型在风控决策体系里的应用架构。目前我所见到有两种主流的风控决策应用架构：策略规则+评分模型 &amp; 策略规则+模型规则。 策略规则+评分模型# 前者策略规则和评分模型是分开的，一般风控流程是先进行策略规则的风险判断，再进入评分模型的风险识别； 策略规则+模型规则# 后者是将评分模型的预测概率（或分数）转变为一个策略规则，与其他策略规则融合在一起进行风险决策。 策略规则的粗放式管理# 策略规则作为一种风险识别的方法，其自身具有直观、易用等特性。对于新产品上线前的风险决策，因为没有数据样本的原因，策略规则在风险决策初期起到不可替代作用。但也因为策略规则的设定原理，其自身很难做到风险决策的精细化管理。 以上图风险决策B为例，可以看出策略规则都是XXXX&gt;xxx，这种单维度的风险判断是存在一定的取舍。比如某金融机构的一条多头借贷策略规则设定为：多头借贷平台数&gt;5则执行拒绝，那多头借贷=6的申请客户，就一定会违约吗？ 说到这里可能会有读者朋友质疑：我可以设定一些策略规则组合起来判断。没错，这也是风险决策体系下策略规则应用的一种方式，但不论多少维度的组合判断，都必然会对单一维度策略规则进行True or False判断。比如上例中的策略规则变为：多头借贷&gt;5 或 多头借贷&gt;6且性别为男性，则执行拒绝。此时对于多头借贷=6的女性不会拒绝，但对于多头借贷=7且有一定储蓄的男性，就一定会违约吗？ 可以看出，如果希望通过策略规则的组合实现精细化的风险管理，就会不断地增加策略规则，最终导致策略规则的复杂和冗余，对于策略优化、回顾并没有正向的影响，这与策略规则的易用、直观等特性产生了矛盾。 评分模型的常见三种盲区# 由于策略规则的先天性缺陷，评分模型的出现可以恰当的弥补策略规则的不足，但并不意味着评分模型可以完全替代所有的策略规则。其原因有风控流程的考虑、业务发展的考虑等，在本文我为大家从模型自身的盲区为大家作解释。 建模数据集与实际贷款人之间存在偏差# 在中国因为征信体系的不完善，金融机构的模型一般以实际贷款人作为模型数据集，而申请人母集到贷款人子集往往发生较大变化（就算是大家熟知的拒绝推断也只能尽量弥补但不能完全拒绝这方面的误差），模型的判断就会出现一些偏差，此时需要根据策略维度的一些拒绝线，对模型进行一些矫正和保护。 模型数据集来自历史，与未来实际情况存在偏差# 模型是基于历史数据找到数据之间的逻辑规律后，对未来事件进行预测。对于具有周期性的金融行业，如果用处于上升期的数据模型预测金融衰退期的事件，必然会与实际情况发生偏差。 举个例子，比如在经济上升或者繁荣期，消费者不仅有工作的单一收入，消费者可以从一些兼职等渠道获取额外的收入来源，此时即使有较高负债收入比的客群仍然可以维持较好的信用表现；但当经济开始进入下滑时期，未来消费者很难继续从其他渠道获取资金，即使历史数据告诉模型、模型告诉决策人，此时的借贷申请人有还款能力和意愿，但商业风险决策者应考虑收紧对于较高负债收入比人群的贷款。 模型对于目标变量的界定与实际商业目标存在偏差# 模型为了权衡观察期的代表性和表现期的时效性，在建模时为了囊括最近的贷款数据，在界定“坏账”定义时，仅考虑前12个月的还款表现（有时仅考虑前6个月），此时对于一些中额长期的信贷产品（比如24个月、36个月），模型目标变量的界定与实际商业目标就发生了偏差。 综上，从反面辩证性的角度分析模型与策略，二者缺一不可，谁也不可能完全替代对方。通过科学地搭配，共同构架起严谨的风险决策体系。","categories":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/categories/risk/"}],"tags":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/tags/risk/"},{"name":"strategy","slug":"strategy","permalink":"https://blog.sofunnyai.com/tags/strategy/"}]},{"title":"互联网金融资产质量评估指标---fpd、vintage、rollrate和迁移率等等","slug":"风控指标vintage和rollrate","date":"2020-03-21T17:25:33.000Z","updated":"2020-05-25T06:57:03.990Z","comments":true,"path":"article/vintage_rollrate_fpd.html","link":"","permalink":"https://blog.sofunnyai.com/article/vintage_rollrate_fpd.html","excerpt":"","text":"目录# 概述 基础概念 风险分析有常见的5要素： 其他的相关概念 延滞率 即期指标（coincidental）： 递延指标（lagged）： 账龄(Month of Book，MOB) FPD 首逾率（反欺诈相关） 入催率 逾期时间 vintage 成熟期 观察点与表现期： vintage举例 roll rate滚动率 滚动率定义—状态变化 如何确定bad和good标签的账龄 vintage和roll rate的区别： roll rate已经确定了bad为什么还需要通过Vintage分析来确定表现期？ Vintage里所有的账户，我们的目的是抓住尽可能多的坏客户。 Flow Rate迁移率 坏账准备金 坏账准备金的概念----各期逾期余额乘以各自准备金率 准备金比例 资产组合管理相关 参考文章链接： 概述# 想写一个vintage和roll rate的文章，查到求是汪大佬写的非常详细，图文并茂。直接偷懒摘录了一个脱水版，想看原文的在这里。下面开始干货。 本文主要讲述账龄vantage、滚动率roll rate、迁移率flow rate三个指标的关系。 账龄分析（Vintage Analysis）：用以分析账户成熟期、变化规律等。 滚动率分析（Roll Rate Analysis）：用以定义账户好坏程度。 迁移率分析（Flow Rate Analysis）：用以分析不同逾期状态之间的转化率。 基础概念# 风险分析有常见的5要素：# 下单月： 观察月： 放款额（GMV）：就是零售业说的“流水”，成交总额包括销售额、取消订单金额、拒收订单 金额和退货订单金额。 在贷余额（Balance）/ ENR：至某时点借款人尚未偿还的本金 逾期天数（DPD） 其他的相关概念# C,M1,M2,M3…的贷款余额：根据逾期期数(C,M1,M2,M3…)，计算每条借款的当时的贷款余额。（贷款余额 = 放款时合同额 –已还本金） 核销金额: 贷款逾期M7+（坏账）后经审核进行销帐，核销金额即在核销日期当天的贷款余额。 回收金额 Recovery：来自历史所有已核销合同的全部实收金额。（核销后又回收的部分） 净坏账 NCL：当月新增核销金额 – 当月回收金额（坏账减去回收） 延滞率# 延滞率（delinquent%）的计算可分为coincidental以及lagged两种方式，除了各bucket延滞率之外，也会观察特定bucket以上的延滞率，如M2+lagged%以及M4+lagged%等指标，以M2+lagged%为例，分母为两个月前应收账款，分子为本月M2（含）以上尚未转呆账的逾期金额。 在消费金融风险管理上，M2以及M4为两个重要的观测点，原因是客户可能因为一时忙碌或疏忽造成账款逾期，但若经过M1催收仍旧落入M2以上，几乎可以确认为无力缴款或者蓄意拖欠。另外依据经验，客户一旦落入M4，事后转呆账几率非常高。 下面部分来自知乎京东白条的回复： 逾期率有两种方法：即期逾期率指标Coin(X)%和递延逾期率指标Lagged(X)% 这个指标分子 = 时点逾期余额，分母 = 时点透支余额。逾期率最直观，但也最容易被“操控”： 分子会受到统计粒度的影响（以客户/账户/贷款单/贷款分期单哪个粒度来计算差异很大，尤其是偏重分期的产品），也会受到核销等资产处置的影响（是否有核销处理、统计时是否包含）。 分母会受到当期时点规模的影响，季末/年末冲量或特殊的营销时点上，都会导致规模的激增，人为拉低时点的逾期率。 在产品高速发展时期，规模增长迅速，而风险滞后释放，导致对风险的低估和滞后判断；而在产品成熟和衰退期，导致对风险的高估，容易误导风险策略的制定。 由于上述原因的存在，一般逾期率只能说明累积的整体风险水平如何。如果根据时点的不良率判断近期风险水平，存在高估/低估的可能。-------------逾期率只能是整体一个粗略的观察，具体来说逾期的结构更重要。 如果一定要用时点余额来判断近期的风险，建议使用延滞率或逾期的净生成率： Mi延滞率=当期Mi/i月前的M0 Mi净生成率=(当期Mi-i月前的Mi)/i月前的透支余额 这两个指标至少能在一定程度上排除当期时点规模、历史存量的影响，能更真实反映近期的风险水平。 新增客户/资产的风险水平如何？# 整体的逾期率并不一定能回答或甄别近期客户/资产的风险水平。需要通过VINTAGE拆解客户和资产，观察相同的表现期后不同客群/资产的逾期占比，例如激活或放款后1~24个月各月月末时点的0+/30+/90+贷款户数和金额或余额的占比。 通过对比不同激活月或放款月在相同的表现窗口后的逾期水平，能观察不同激活月/放款月（对应了不同的策略或人群）的风险走势，更合理评价不同时点人群/资产的情况和策略的效果。 如果运营是针对的客户的，按照激活时间划分客群统计VINTAGE更通用一些；如果是针对资产进行运营，按照贷款的放款月划分资产统计VINTAGE更通用一些。 存量资产的迁移情况？# 除了需要时刻关注新增的风险外，也需要掌握已经逾期资产的迁移/退出情况，也就是滚动率(或者迁徙率、迁移率)。 滚动率一方面体现了客群和资产的质量，也能反映催收运营的状况。 常见的滚动率一般是当期账龄余额与上一期上一账龄余额的比值，用百分比表示。账龄越高，滚动率越高，表示资产回收的可能性越低，进入下一期高账龄的概率越高。 滚动率也可以进一步细分为向上/向下滚动，一般默认的滚动率都是向上滚动，即从低账龄滚到高账龄。向上/向下滚动需要锁定月末时点某一账龄的客户/资产，在下月底观察锁定的这部分客户或资产，统计向上/向下滚动的占比，能排除统计粒度和分期带来的降期影响。 另外，滚动率与产品、前中后的运营等诸多因素有关，短期容易波动，可以计算复合滚动率，例如M0-&gt;M4的滚动率，能从长周期来观察资产质量和运营的稳定水平。 即期指标（coincidental）：# 计算延滞率时常用的两种方法之一，以当期各bucket延滞金额÷应收账款（AR）。 如逾期率Coin©%、Coin(M1)%、Coin(M2)%、Coin(M3)%等等： 当月不同逾期期数的贷款余额/当月底总贷款余额 例： Coin©%=当月C贷款余额/当月底贷款余额(C-M6)-------------------正常用户余额占所有贷款余额 Coin(M1)%=当月M1贷款余额/当月底贷款余额(C-M6) Coin(M1+)%=当月M1−M6贷款余额/当月底贷款余额(C-M6)------------------逾期M1以上的用户余额占所有贷款余额 递延指标（lagged）：# 计算延滞率时常用的两种方法之一，延滞金额÷上月应收账款。若单纯想了解各月资产质量结构，可使用coindental，但若想精准溯及逾放源头的话，建议采用lagged。 与coincident相同也是计算延滞率的一个指标，区别是lagged的分母为产生逾期金额的那一期的应收账款。Lagged观察的是放贷当期所产生的逾期比率，所以不受本期应收账款的起伏所影响。 逾期率Lagged(M1)%、Lagged(M2)%、Lagged(M3)%、Lagged(M4)%、Lagged(M5)%、Lagged(M6)% Lagged DPD30+ = 当前逾期&gt;=30天的客户的本金余额 / 30天前的累计放款本金 当月不同逾期期数的贷款余额/往前推N个月的总贷款余额可以提出当前时点的影响。 例: Lagged(M1)%=当月M1的贷款余额/上个月底时点的贷款余额 -----------------其实就是平台当月M1/1月前的M0 Lagged(M4)%=当月M4的贷款余额/往前推四期的总贷款余额 Lagged(M4+)%=当月M4的贷款余额/往前推四期的总贷款余额 + 当月M5的贷款余额/往前推五期的总贷款余额 + 当月M6的贷款余额/往前推六期的总贷款余额 逾期结算# 实际风险有两种结算方式： Month end：月底结算 （常用方式，主要以自然月月底的逾期指标为主） Cycle end：期末结算（单个借款人还款日时间不同，在月底结算的数据不准确，所以一般设置20日还款，留出10天给催收部门） 早期逾期多数为借款人忘记还款，或短时间资金周转不周，这是与策略密切相关的。通过借款人债偿能力评估识别出借款人有足够资金，可以不做提醒，以获取滞纳金，对于借款人资产表现不好的，可以设置提前10天提醒还款。 逾期天数90-119天，为资产M4阶段。M4-M6的阶段都称之为不良。M4是一个重要的节点，因为消金公司，上报给银监会，或上市公司披露财务数据、风险数据时，都会选择披露不良率。如果有些公司要在审计认可这个方法论时，会对M4做一些调整。 通常180天以上都作为坏账处理，坏账也是被披露的数据之一，还包含一些特别的计提。 账龄(Month of Book，MOB)# 指资产放款月份。放款日截止观察点的月数。如： MOB0：放款日至当月月底 MOB1：放款后第二个完整的月份 MOB2：放款后第三个完整的月份 FPD 首逾率（反欺诈相关）# FPD是指首期逾期率，是说在某一个还款日，仅第一期到期的客户中有多少没有按时还款。与入催率的差别在于，入催率包含了第一期、第二期、第三期等等所有到期的M0。FPD一般用来做反欺诈，因为欺诈用户他第一期是根本不会还款的。 用户授信通过后，首笔需要还款的账单，在最后还款日后7天内未还款且未办理延期的客户比例即为FPD 7，分子为观察周期里下单且已发生7日以上逾期的用户数，分母为当期所有首笔下单且满足还款日后7天，在观察周期里的用户数。常用的FPD指标还有FPD 30。 举例： 假设用户在10.1日授信通过，在10.5日通过分期借款产生了首笔分3期的借款，且设置每月8日为还款日。则11.08是第一笔账单的还款日，出账日后，还款日结束前还款则不算逾期。如11.16仍未还款，则算入10.1-10.30周期的FPD7的分子内。通常逾期几天的用户可能是忘了还款或一时手头紧张，但FPD 7 指标可以用户来评价授信人群的信用风险，对未来资产的健康度进行预估。 与FPD 7 类似，FPD 30也是对用户首笔待还账单逾期情况进行观察的指标。对于逾期30天内的用户，可以通过加大催收力度挽回一些损失，对于逾期30天以上的用户，催收回款的几率就大幅下降了，可能进行委外催收。如果一段时间内的用户FPD 7较高，且较少催收回款大多落入了FPD 30 内，则证明这批用户群的non-starter比例高，借款时压根就没想还，反之则说明用户群的信用风险更严重。 入催率# 有了前面的铺垫，入催率就比较简单了。它指的是在某一个还款日，客户从M0变成M1的比例。比如说，今天，有N个M0客户到了还款日，里面有M个客户按时还款了，那么今天的入催率就是（N-M）/N。它与上面的FPD是有区别的。 逾期时间# DPD、M0（未逾期）、M1（逾期一个月类）、M2（逾期两个月内）。。。 一般M3+就要委外了，M6+(180天以上)就要记为坏账了。 vintage 成熟期# 其实就是逾期率随着账龄变化的趋势图。常见的作用有： 逾期率：vintage的纵轴随着横轴账龄的增大肯定是变大的，最终的平稳后的逾期率（最大值）就是逾期率。（有资产逾期率、账户逾期率两种口径） 欺诈：如果前两期逾期率陡增，短期风险没处理好，是欺诈（特别是第一期就违约的） 信用风险：如果一直上升、很久不拐，说明信用风险控制不太好。 成熟期： 因素判断：风险策略变化、客群变化、市场环境、政策法规变化时，资产质量的变化。（看vintage曲线的波动） 观察点与表现期：# 观察点、观察期、表现期：通常是在整个MOB中选取一个月份作为观察点，前面的期限是观察期，后面的期限是表现期。也就是在时间轴上选取一个点，这个点是观察点。前面的是观察期，后面的是表现期。 表现期越长，信用风险暴露将越彻底，但意味着观察期离当前越远，用以提取样本特征的历史数据将越陈旧，建模样本和未来样本的差异也越大。(模型PSI高) 反之，表现期越短，风险还未暴露完全，但好处是能用到更近的样本。（模型PSI低） vintage举例# 下面是求是汪大佬的一个例子： 对于一个12期分期还款的信贷产品，理论上当用户在12期结束，并还清所有的钱后，我们才能定义为绝对的好客户；反之，我们只能说到目前为止是一个好客户，但并不能知道未来几期用户会不会逾期不还钱。 汪大佬的这个图中可以看到： 账龄最长为12个月，代表产品期限为12期。 根据2018年5月放贷的订单完全走完账龄生命周期，而2018年6月却没走完，说明数据统计时间为2019年6月初。 账龄MOB1、MOB2、MOB3的逾期率都为0，说明逾期指标为M4+（逾期超过90天）风险。 由放贷月份从2018年1月～12月的账户的最终逾期率都在降低，说明资产质量在不断提升，可能是因为风控水平在不断提升。 2018年5月相对于2018年1～4月的逾期率大幅度下降，说明该阶段风控策略提升明显。 不同月份放款的M4+在经过9个MOB后开始趋于稳定（后面违约率不再大幅上升），说明账户成熟期是9个月。 roll rate滚动率# 滚动率定义—状态变化# 滚动率：就是从某个观察点之前的一段时间**（观察期）的最坏的状态，向观察点之后的一段时间（表现期）的最坏的**状态的发展变化情况。（就是说从上一状态向下一状态发展的一个度量，说最坏是因为可能用户逾期多笔） 滚动率分析的具体操作步骤为： step 1. 确定数据源。一般利用客户还款计划表（repayment schedule）。 step 2. 选择观察点，以观察点为截止时间，统计客户在观察期（如过去6个月）的最长逾期期数，按最坏逾期状态将用户分为几个层次，如C（未逾期）、M1、M2、M3、M4+。 step 3. 以观察点为起始时间，统计客户在表现期（如未来6个月）的最长逾期期数，按最坏逾期状态将用户分为几个层次，如C、M1、M2、M3、M4+。 step 4. 交叉统计每个格子里的客户数，如图6中表1所示。 step 5. 统计每个格子里的客户占比，如图6中表2所示。 step 6. 为了排除观察点选择时的随机影响，一般会选择多个观察点。重复step1 ～5。 上面的图说明： 逾期状态为M0的客户，在未来6个月里，有96%会继续保持正常状态，4%会恶化为M1和M2； 逾期状态为M1的客户，未来有81%会回到正常状态，即从良率为81%，有7%会恶化，13%会保持M1状态； 逾期状态为M2的客户，从良率为23%，有39%会恶化为M3和M4+； 逾期状态为M3的客户，从良率为14.7%，有60.7%会恶化为M4+； 逾期状态为M4+的客户，从良率仅为4%，有80%会继续保持此状态。 因此，我们认为历史逾期状态为M4+的客户已经坏透了，几乎不会从良。为了让风控模型有更好的区分能力，需要将客户好坏界限尽可能清晰(也就是从良率最剧烈减少的点开始)，可以定义： 坏用户（bad）= 逾期状态为M4+（即逾期超过90天） 如何确定bad和good标签的账龄# vintage和roll rate的区别：# 滚动率分析用于定义客户的好坏程度。(定义标签bad和good) Vintage分析用于确定合适的表现期。（找到一个合适的观测点，前面的该逾期的已经逾期了，充分暴露了） 定义目标客户到底是good还是bad的具体操作步骤为： step 1. 利用滚动率分析定义坏客户（找到不会再从良的那个账龄点），例如上文案例中定义：M4+为坏客户。（先找到bad和good） step 2. 以M4+作为资产质量指标（上一步找到定义了bad还是good），统计Vintage数据表，绘制Vintage曲线。目的是分析账户成熟期（逾期率不再明显增加），例如上文案例确定：账户成熟期是9个月。 也有根据迁徙率确定bad还是good的，下面是FAL提到的一个例子： 由下表可以看出，M2以上的迁徙率将近90%，所以确定当前逾期31天以上为区分好坏客户的标准，及后续分析的目标变量。 roll rate已经确定了bad为什么还需要通过Vintage分析来确定表现期？# 这是因为：虽然滚动率分析确定了M4+作为坏的程度（从良率最低），但是对于12期的产品，有些账户是在前4期MOB（也就是MOB1 ~ MOB4，经过4个表现期）就达到M4+，有些是在（观测点前）后几期才达到M4+。而这很重要。 Vintage里所有的账户，我们的目的是抓住尽可能多的坏客户。# 现在进一步补充Vintage曲线的绘制过程：如图8所示，对于这10,000个账户，以MOB1为起点，把前N个MOB作为一个窗口，滑窗统计坏客户率，得到图5-表1中的Vintage数据，并绘制Vintage曲线。我们可以发现：经过9期，我们几乎能够抓住所有的坏客户。（也就是前9期该逾期的都逾期了，充分暴露了） 下图是每个用户逾期的不同起止情况举例： 因此，我们将两者结合起来，定义： Bad = 账户经过9期表现期后，逾期状态为M4+（逾期超过90天）。此时 Y=1。 Good = 经过9期表现期，但未达到M4+逾期状态。此时Y=0。 Intermediate = 未进入9期表现期，账户还未成熟，无法定义好坏，也就是不定样本。 有时候也考虑到这么干的话，bad用户会太少，会往上移动到M3,同时因为前面的good用户要和bad做一个截断。比如M1以内的都是good，m3+的都是bad，m2的忽略截断。 Flow Rate迁移率# 展示客户贷款账户在整个生命周期中的变化轨迹，也是预测未来坏账损失的最常用的方法。 其核心假设为：处于某一逾期状态（如M2）的账户，一个月后，要么从良为M0账户，要么恶化为更坏的下一个逾期状态（如M3）。 迁移率 = 前一期逾期金额到下一期逾期金额的转化率 一般缩写为M0-M1、M4-M5等形式，例如： M0-M1 = 当月进入M1的贷款余额 / 上月末M0的贷款余额 M2-M3 = 当月进入M3的贷款余额 / 上月末M2的贷款余额 迁移率分析的具体操作步骤为： step 1. 定义逾期状态，如前文所述的M0、M1、M2等。 step 2. 计算各逾期状态之间的迁移率，如M0-M1、M2-M3等。 step 3. 计算不同月份（也可称为Vintage）的平均迁移率。目的是对本平台在不同时期的资产的迁移率有整体的认知。 step 4. 根据平均迁移率和不良资产回收率，计算净坏账损失率。 接下来，我们以数值案例（非真实业务数据）展示上述过程。 下面表1，每一列代表截止当月放款的总额M0一直到M6的情况，每一行代表1-7月的各个月对应周期的违约率,所以斜线是前后时间序列关系： 上图表2中，2月份的逾期M1资产只能从1月份的正常M0资产滚动而来（斜线迁移），因此从逾期M0资产向M1的转化率为: 2373381007844=23.55%\\frac{237338}{1007844}=23.55\\% ​1007844​​237338​​=23.55% 截止1月末，正常M0资产为 1007844 元，这是起点。 截止2月末，1月末的正常M0资产中有2373381007844=23.55%\\frac{237338}{1007844}=23.55\\%​1007844​​237338​​=23.55% 恶化为逾期M1资产。【较低，因为有不小心逾期的】 截止3月末，2月末的逾期M1资产中有 \\frac{55362}{237338}=23.33%恶化为逾期M2资产。【较低，因为有不小心逾期的】 截止4月末，3月末的逾期M2资产中有 \\frac{25144}{55362}=45.32% 恶化为逾期M3资产。【翻倍上升了】 截止5月末，4月末的逾期M4资产中有83.38%恶化为逾期M5资产。此时已过催收黄金期（90天以内）。【大幅上升！】 截止6月末，5月末的逾期M5资产中有49.37% 恶化为逾期M6资产。这可能采用了委外催收、司法手段等催收策略，效果显著。【从80+%下降到49%】 截止7月末，6月末的逾期M5资产中有82.7% 恶化为逾期M7资产。此时将视为不良资产，打包转卖给第三方公司，这样就能回收部分不良资产，减少损失【没救了】 我们从横向比较每个月的迁移率，发现不完全一样。这是因为随着时间推移、外在宏观经济环境、用户渠道、内部政策、资产质量等变化而产生一定的波动。我们可以利用这些数据： 观察迁移率的发展轨迹，监控坏账的发展倾向和催收效果。 通过对多个月份的迁移率计算平均值，从而使迁移率更加稳定。 坏账准备金# 坏账准备金的概念----各期逾期余额乘以各自准备金率# 呆帐风险是信贷机构必须面对的风险，主要来源于信用风险和欺诈风险等。为了应对未来呆帐的可能，信贷机构一般都会设定一个储备资金，这就是**坏账准备金（Bad Debt Reserve）。**那么我们该如何计算坏账准备金？ 一般做法是，把未清偿贷款余额乘以一定的**准备金比例（Reserve Ratio）**所得。可以理解，资产逾期等级越高（越差），准备金比例也应该越高，因为恶化为呆帐的可能性也更高。如图10所示，正常M0资产恶化为呆帐的可能性最低，因此我们预留的准备金比例也就最少。 我们总结下计算坏账准备金的步骤为： step 1. 统计未清偿贷款金额的分布，也就是M0~M6状态分别对应的资产余额。 step 2. 为每个逾期状态的资产分配一个准备金比例。 step 3. 每个子项目的准备金金额 = 未清偿贷款余额 x 准备金比例。 step 4. 每个子项目的准备金金额相加，得到最终的准备金。 准备金比例# 准备金比例是如何给出的？ 由于坏账准备金是用来覆盖预期的未来呆帐损失的，准备金比例必须等于处于各个逾期状态的资产未来演变为呆帐的比例。 回到上一节的迁移率分析中，我们发现从正常M0资产迁移至逾期M7资产（呆帐）需经过7次迁移，如图11所示。那么，我们只要把各个状态之间的转化率相乘，不就得到准备金比例了？ 因此，我们定义正常M0资产对应的毛坏账损失率，也就是迁移到呆帐的转化率为： 毛坏账损失率 = (M0-M1)×(M1-M2)×(M2-M3)...×(M6-M7) 也就是从M0一直到M7的平均迁移率的乘积。 在本案例中，正常M0资产对应的毛坏账损失率为上上图表2最左侧截止M6-M7的平均迁移率88.03%上面所有的乘起来：0.60% 在实际中，信贷机构会将不良资产打包转卖给第三方公司，这样就能回收部分不良资产，减少损失。因此，我们定义净坏账损失率为： 净坏账损失率 = 毛坏账损失率 - 不良资产外卖回收率 由于M7不良资产的平均回收率为 10.79%，则可计算净坏账损失率为： 0.60%×(1-10.79%)=0.54% 同理，我们可以计算正常资产到不同逾期状态资产的毛损失率和净损失率如下： 根据图12所示的损失率表，我们定义： 当月应计拨备额 = SUM(净坏账损失率 * 月末应收账款余额) 拨备率 = 当月应计拨备额 / 总资产金额 其中，拨备率是用来预防不良资产的发生而准备的金额的比例。拨备率应越低越好。拨备率越高说明风险越大，损失越大，利润越小。 在本案例中，当月应计拨备额为65421元，如图13所示。拨备率为： 654212625091=2.49%\\frac{65421}{2625091}=2.49\\% ​2625091​​65421​​=2.49% 资产组合管理相关# 根据风控成因分类：信用风险、欺诈风险。信用风险主要是用户因为各种原因导致逾期而存在的风险，欺诈风险就是黑产欺诈团队的攻击对公司造成的风险，通过设置规则来拦截高风险用户。 生命周期分为三类： 拓展客户期（学校刚成立时，既要招生，又要有教材支撑） 审批客户期（学习成绩、平时表现） 管理客户期（对学生进行管理） 拓展客户期需要三个方面的支持 目标用户： 适用于拨备segment的风险分级或用户画像支持（拨备与财务挂钩） 目标产品： 风险分级对应期数、利率支持 资产配置有效性分析 资金成本、获客成本、运营成本，在放贷还没开始的时候，就已经由资产管理部门估算确定下来了。后续需要技术来创造价值的，主要是风控的坏账成本，所以资产管理部会用拨备工具来给予支持。 在一个产品刚产生的时候，资产管理部门需要给出关于目标客户的年龄身份，期数，利率的建议；放款后，又需要从其他金融机构拉资金，债权转让等。每一个公司的要求都不一样，我们需要给出推荐，哪一些标的推荐给哪一些公司，如何进行资源组合配置。 审批客户期 主要由贷前策略实施，资产组合管理部门可提供盈利性测算支持，并做好监控、预测、预警系统，当准入用户风险状况超阈值，需提出干预。 资金成本、获客成本、运营成本，在放贷还没开始的时候，就已经由资产管理部门估算确定下来了。后续需要技术来创造价值的，主要是风控的坏账成本，所以资产管理部会用拨备工具来给予支持。 在一个产品刚产生的时候，**资产管理部门需要给出关于目标客户的年龄身份，期数，利率的建议；**放款后，又需要从其他金融机构拉资金，债权转让等。每一个公司的要求都不一样，我们需要给出推荐，哪一些标的推荐给哪一些公司，如何进行资源组合配置。 审批客户期 主要由贷前策略实施，资产组合管理部门可提供盈利性测算支持，并做好监控、预测、预警系统，当准入用户风险状况超阈值，需提出干预。 管理客户期 1.指标方面：新增/存量、风险/规模指标 风险 规模 新增 vintage、FPD GMV 存量 roll rate、coincident dpd、lagging dpd、badrate 在贷余额 2.策略方面：主要由贷中贷后策略实施，可提供盈利性测算支持，并做好监控、预测、预警系统，风险状况超阈值，需提出干预。 资产组合管理作为支撑部门，支撑什么？ 风险计量 策略规则上线 模型效力验证 向CRO提供各种专题类或临时性分析 … 风险计量主要是数据分析，报表，专题性报告，为了规避、减少风险，策略实施者和策略制定者需要分为2个部门。资产管理部门接到业务部门提交的需求，然后根据内容做一些空跑，监控。同时大的模型开发与模型验证也是两个部门，需要资产管理部对模型做持续的监控与评估。统筹贷前、贷中、贷后的数据给到CRO。 资产组合管理方法？ 1.拨备准备金 思考：实际风险与名义风险的区别？ 2.风险分级(用户画像) 思考：有了评分卡模型为什么还要做风险分级? 3.监控、预测、预警系统 思考：资产组合管理报表和业务部门（贷前、中、后）的不同点？ 本次分享总结 1.资产组合管理部门不同于传统的风控业务部门，而是直属于CRO的信息整合部门； 2.资产组合管理贯穿于客户及产品的全生命周期，需要从业者极强的沟通能力； 3.作为支撑部门，资产组合管理部需要运用数据分析把控公司全局资产质量，除此之外还需要以降低作业风险的目的为模型或策略设置二次防线。 4.资产组合管理部门是小白进入金融风控核心岗位的捷径，可以快速的积累风控经验，之后如果转岗策略或模型，都比较容易。 参考文章链接：# https://zhuanlan.zhihu.com/p/81027037/ https://blog.csdn.net/liulj_0803/article/details/52964473 https://www.zhihu.com/question/51583052 这里也有很多干货","categories":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/categories/risk/"}],"tags":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/tags/risk/"},{"name":"评分卡","slug":"评分卡","permalink":"https://blog.sofunnyai.com/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"}]},{"title":"SpringCloud H版上手测试----01CAP、注册中心、Ribbon实践","slug":"springcloud01-CAP、注册中心、Ribbon实践","date":"2020-03-02T14:30:40.000Z","updated":"2020-06-02T03:53:04.772Z","comments":true,"path":"article/springcloud-H-01-eureka-ribbon.html","link":"","permalink":"https://blog.sofunnyai.com/article/springcloud-H-01-eureka-ribbon.html","excerpt":"","text":"CAP理论快速简介 微服务的通用定义： 本次测试版本： 分布式服务包括的组件 环境搭建 支付子module建立： IDEA自动热部署配置（可选） 关于RestTemplate 注册中心 EUREKA服务注册 工作流程 Server配置 client配置 EUREKA高可用集群 服务集群 自我保护 Discovery Zookeeper服务注册 注入调用方 zk客户端和server jar包冲突 Consul服务注册 简介 服务端注册 客户端注册 Eureka、Zookeeper、Consul的对比 Ribbon Ribbon简介 Ribbon的负载均衡策略 其他负载均衡算法： 自定义规则 注意事项： 原始的轮询规则 定义手动实现一个负载均衡 动手实现一个自定义ribbon策略 CAP理论快速简介# CAP理论关注的是粒度是数据，而不是整体系统设计。 Consistency（强一致性） Availability（高可用性） Partition Tolerance（分区容错性） 以上三个最多只能较好满足两个，一个系统不可能同时满足这三个需求。 CA：单点集群，满足一致性和高可用，通常在可拓展性上不强大。 CP：满足一致性，分区容忍度。通常性能不太高。 当网络分区出现后，为了保证一致性，就必须拒接请求，否则无法保证一致性。（违背A高可用） Consul和Zk，更关注一致性，不在了很快就给干掉，但不是立刻。 AP：满足可用性和分区容忍度，通常对一致性要求较低。 当网络分区出现后，为了保证可用性，B系统可以返回旧的值，优先保证可用。（违背C一致性） Eureka，不会立刻踢掉服务 分布式架构的P永远都要保证 图例 AP：如微博热门微博点赞数，后续柔性理论和base数据补充来保证一致性 CP： 微服务的通用定义：# 是一种架构模式，提倡将单一应用划分成一组轻量级的微服务互相调用和配合，基于restful，并可以独立部署。 是一整套的较量，不是单个的组件。 本次测试版本：# springboot 2.2.x+版本 spring cloud H版。如果cloud是G，boot对应2.1具体： boot 2.2.2.RELEASE： CLOUD Hoxton.SR1 Cloud Alibaba 2.1.0.RELEASE 分布式服务包括的组件# 服务注册与发现：eureka，现在不维护了。zk、Consul(golang不推荐)、用alibaba的Nacos(推荐) 服务调用：Ribbon(维护状态)，后续LoadBalancer。Feign用OpenFeign。 服务熔断：hystrix(维护状态，但是大规模，思想需要学习)，resilience4j(海外，国内很少)，alibaba sentienl（推荐） 负载均衡：fegin 服务降级：hystrix 服务消息队列： 配置中心管理：config，推荐携程的阿波罗，或者alibaba的Nacos(推荐) 服务网关：zuul，现在用cloud gateway 服务监控 总线：bus—&gt;alibaba Nacos 全链路监控 自动化部署 服务定时操作 分布式配置：cloud config 环境搭建# idea里面new-project-maven_architect_site 设置项目encoding utf-8 设置项目 settings-annotation processor- 勾选 enable annotation prosessing 设置java-compile是1.8 设置pom：maven项目的的聚合、依赖、传递依赖 父工程，project根目录下的pom文件修改maven，添加packaging标签为pom。 1&lt;packaging&gt;pom&lt;/packaging&gt; dependencyManagement用在父工程，子模块继承后，提供作用：锁定版本、子module都引用一个依赖，而不用写version。 dependencyManagement用在父工程只是声明版本依赖，并不真的引用。真正的是要子项目自己引用group和artifactid即可，不用指定版本号自动用父类的。 支付子module建立：# 建module：在父工程右键新建module，新建完了之后父工程上会有&lt;module&gt;引入了 改pom 写yml 主动启动类 业务类 IDEA自动热部署配置（可选）# 在子项目工程pom中添加devtools依赖包： 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 添加插件到父聚合项目的pom中 12345678&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; 开启自动编译 更新值 ctrl+alt+shift+/-----registry，勾选允许自动编译： 勾选下面两个选项 重启idea 关于RestTemplate# getForObjec：返回Json对象，或者Json字符串可以格式化为对象。 getForEntity：返回ResponseEntity，除了数据，还包括网络层面的状态码、响应头等东西。 RestTemplate已经被Springcloud深度定制，底层可以支持各种客户端负载均衡策略支持，也支持自定义负载均衡策略。 注册中心# EUREKA服务注册# 传统一对第一调用，太多的时候就是网状的，需要每个客户端都去维护对端服务信息。无法统一管理，非常乱。 EUREKA作为一个注册中心的server，系统中其他services都向他链接注册并维持心跳。 这样EUREKA就能知晓所有的services的信息，就像一个电话号码本。其他service想互相调用可以来这里用service别名来询问当前可用的对端地址。 下面左边是SpringCloud，右边是Dubbo。 EUREKA分为Server和Client两个组件： Server提供一个监听，供给其他cloud所有service进来连接。 Client是一个Java客户端，简化与Server的交互，是一个内置、轮循的负载均衡器。默认30s向Server发送一次心跳。如果Server多伦没有收到心跳就把这个节点移除。(默认90s) EUREKA已经停止更新，后续需要迁移到别的技术栈，比如zk、consl、nacos。 工作流程# Server配置# pom.xml 2.2版本后，eureka分为server和client了，此处是server。 不用指定版本，因为父module中指定了版本。 12345&lt;!--eureka server--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; application.yml 12345678910server: port: 7001eureka: instance: hostname: localhost # erureka client: register-with-eureka: false # 不向注册中心注册自己 fetch-registry: false # 自己仅仅作为注册中心，不去检索服务 service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka Server的main类 12345678@SpringBootApplication@EnableEurekaServerpublic class EurekaMain7001 &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMain7001.class, args); &#125;&#125; client配置# pom.xml 同上 123456&lt;!--eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;!--上面的eureka-client会自动引入ribbon--&gt; application.yml 123456eureka: client: register-with-eureka: true # 是否注册到eureka server 默认true fetch-registry: true # 是否从eureka抓取已注册的信息，true才能配合ribbon使用负载均衡 service-url: defaultZone: http://localhost:7001/eureka Client的main类 1234567@SpringBootApplication@EnableEurekaClientpublic class PaymentMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8001.class, args); &#125;&#125; EUREKA高可用集群# 高可用原理：相互注册，相互守望。 比如7001和7002有两个EUREKA，会互相注册到对方那边去。互相心跳监控。 两台EUREKA的yml配置 12345678910server: port: 7001eureka: instance: hostname: eureka7001.com # erureka另一个节点的ip，互相注册 如192.168.1.2:7001 &lt;---&gt; 192.168.1.3:7002,此处就应该写192.168.1.2 client: register-with-eureka: false # 不向注册中心注册自己 fetch-registry: false # 自己仅仅作为注册中心，不去检索服务 service-url: # 下面应该注册到另外一台eureka，也就是http://192.168.1.3:7002/eureka defaultZone: http://eureka7002.com:7002/eureka 12345678910server: port: 7002eureka: instance: hostname: eureka7002.com # erureka另一个节点的ip，互相注册 如192.168.1.2:7001 &lt;---&gt; 192.168.1.3:7002,此处就应该写192.168.1.3 client: register-with-eureka: false # 不向注册中心注册自己 fetch-registry: false # 自己仅仅作为注册中心，不去检索服务 service-url: # 集群模式下，下面应该注册到另外一台eureka，也就是http://192.168.1.2:7001/eureka defaultZone: http://eureka7001.com:7001/eureka 客户端的yml配置 1234567eureka: client: register-with-eureka: true # 是否注册到eureka server 默认true fetch-registry: true # 是否从eureka抓取已注册的信息，true才能配合ribbon使用负载均衡 service-url: # defaultZone: http://localhost:7001/eureka defaultZone: http://eureka7001.com:7001/eureka,eureka7002.com:7002/eureka # EUREKA是集群 服务集群# 当一个服务在多台机器上运行，注册到一个EUREKA中后，在EUREKA上可以看到服务的多个ip列表用逗号隔开的。 这时候假设客户端还是使用restTemplate请求的，不能写死对端服务的ip和端口，可以写EUREKA中的服务名。 这样消费端不再关注提供方的地址，而且有负载均衡功能 12345678910111213// private String url = \"http://localhost:8001\"; // 单机private String url = \"http://CLOUD-PAYMENT-SERVICE\"; // cloud服务名，会UNKWNON HOST，需要给restTemplate开启LoadBalanced /** * 添加用户 * post http://localhost:8080/customer/payment/add?serial=cus_lalala * @param payment * @return */ @PostMapping(\"/customer/payment/add\") public CommonResult&lt;Payment&gt; create(Payment payment)&#123; return restTemplate.postForObject(url+\"/payment/add\", payment, CommonResult.class); // &#125; 但是因为服务方是多个节点，所以需要restTemplate开启负载均衡功能去调用: 12345678910111213@Configurationpublic class ApplicationContextConfig &#123; /** * 类似&lt;bean id=\"xxx\", class=\"xxxx\"&gt;&lt;/bean&gt; * @return */ @Bean @LoadBalanced //这个注解可以负载均衡，也可以把host地址由一个cloud的application-name去zookeeper转换为ip和端口 public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125;&#125; 自我保护# 一句话描述：某时刻某个微服务不能用了，Eureka不会立刻清理，依旧会对该服务的信息进行保存。 为什么？防止Eureka Server网络不通，但是Eureka Client正常运行的时候，EurekaServer不会立刻把EurekaClient剔除。 详细：默认30s一次，当90s没收到心跳就该干掉，但是如果短时间内大量丢失客户端时，这个节点就会进入自我保护机制。（此时可能大量客户端都是正常的，很可能是网络分区故障） 属于CAP里面的AP分支。（高可用、分区容错性） server端关闭自我保护，修改 123server: enable-self-preservation: false # 关闭自我保护，客户端90s没心跳立马干掉 eviction-interval-timer-in-ms: 60000 Discovery# 主启动类，开启Discovery的能力 12345678@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClient //主启动类，开启Discovery的能力public class PaymentMain8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentMain8001.class, args); &#125;&#125; controller，注入，获取服务和实例信息 1234567891011121314151617@Resourceprivate DiscoveryClient discoveryClient;@GetMapping(value = \"/discovery\") @ResponseBody public CommonResult&lt;Object&gt; discovery()&#123; System.out.println(\"-------------Services--------------\"); discoveryClient.getServices().forEach(System.out::println); System.out.println(\"-------------Instances--------------\"); List&lt;String&gt; instances = discoveryClient.getInstances(\"CLOUD-PAYMENT-SERVICE\").stream().map(each-&gt;each.getInstanceId()+\"/\"+each.getHost()+\":\"+each.getPort()+\"/\"+each.getUri()).collect(Collectors.toList()); instances.forEach(System.out::println); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"services\", discoveryClient.getServices()); map.put(\"instances\", instances); return CommonResult.success(\"discovery\",map); &#125; Zookeeper服务注册# zookeeper使用临时节点存储服务的信息，一会儿心跳不出现就会干掉这个节点（不是立马干掉）。是CAP的CP，和EUREKA不太一样。 服务引入jar包 12345&lt;!--zookeeper client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt; &lt;/dependency&gt; application.yml只有几行不一样，指定zookeeper的连接字符串即可： 123456789server: port: 8004spring: application: name: cloud-provider-service cloud: # 启动基于zookeeper的服务注册 zookeeper: connect-string: 127.0.0.1:2181 主启动类，使用DescoveryClient即可 12345678910111213@SpringBootApplication@EnableDiscoveryClientpublic class PaymentZkMain8004 &#123; /** * 基于zk的服务提供方，访问/payment/zk即可看到测试效果 * @param args */ public static void main(String[] args) &#123; SpringApplication.run(PaymentZkMain8004.class, args); &#125;&#125; 启动后： 123456789101112[zk: localhost:2181(CONNECTED) 7] ls /[services, zookeeper][zk: localhost:2181(CONNECTED) 8] ls /services[cloud-provider-service][zk: localhost:2181(CONNECTED) 10] ls /services/cloud-provider-service[ba35208f-59ea-4d70-b75f-857c5a5b0a64][zk: localhost:2181(CONNECTED) 11] get /services/cloud-provider-service/ba35208f-59ea-4d70-b75f-857c5a5b0a64&#123;\"name\":\"cloud-provider-service\",\"id\":\"ba35208f-59ea-4d70-b75f-857c5a5b0a64\",\"address\":\"192.168.1.10\",\"port\":8004,\"sslPort\":null,\"payload\":&#123;\"@class\":\"org.springframework.cloud.zookeeper.discovery.ZookeeperInstance\",\"id\":\"application-1\",\"name\":\"cloud-provider-service\",\"metadata\":&#123;&#125;&#125;,\"registrationTimeUTC\":1590931922948,\"serviceType\":\"DYNAMIC\",\"uriSpec\":&#123;\"parts\":[&#123;\"value\":\"scheme\",\"variable\":true&#125;,&#123;\"value\":\"://\",\"variable\":false&#125;,&#123;\"value\":\"address\",\"variable\":true&#125;,&#123;\"value\":\":\",\"variable\":false&#125;,&#123;\"value\":\"port\",\"variable\":true&#125;]&#125;&#125;[zk: localhost:2181(CONNECTED) 12] 注入调用方# pom和application.yml和启动类一模一样，暂时用restTemplate调用，需要config一下 12345678910@Configurationpublic class ApplicationContextConfig &#123; @Bean @LoadBalanced //这个注解可以负载均衡，也可以把host地址由一个cloud的application-name去zookeeper转换为ip和端口 public RestTemplate getRestTemplate()&#123; RestTemplate template = new RestTemplate(); return template; &#125;&#125; customer的controller 123456789101112131415161718@RestControllerpublic class ZkCustomerController &#123; @Resource private DiscoveryClient discoveryClient; @Resource private RestTemplate restTemplate; public static final String URL = \"http://cloud-provider-service\"; @GetMapping(\"/customer/payment/zk\") public CommonResult&lt;Payment&gt; testZkCloud()&#123; return restTemplate.getForObject(URL+\"/payment/zk\", CommonResult.class); &#125;&#125; 启动后可以在zk看到双方： 12[zk: localhost:2181(CONNECTED) 26] ls /services[cloud-customer-order, cloud-provider-service] zk客户端和server jar包冲突# 有可能client和server的zk jar包不一致会报错，比如客户端太新，服务端太老。 需要在cloud的zk的starter里面exclude掉zk的包，然后重新引入一个和zkServer版本一致的包即可。 Consul服务注册# 简介# 分布式的服务注册和配置管理中心(KV存储)，同时提供控制总线由golang开发。 基于Raft协议，比较简洁。支持健康检查、Http和DNS协议，支持跨数据中心的WAN集群，支持图形界面、跨平台。 功能： 服务发现：Http和DNS两种方式 健康检查：多种方式，Http、TCP、Docker、Shell定制 KV存储：K-V存储，可以做配置管理 可视化界面 官网下载解压，只有一个consul文件 https://www.consul.io/ 启动参考video： https://learn.hashicorp.com/consul/getting-started/agent 比如consul agent -dev启动 访问http://localhost:8500查看信息，或者： 1234567891011121314151617181920 curl localhost:8500/v1/catalog/nodes # 获取消息[ &#123; \"ID\": \"890e9cd0-322b-fafc-fe58-33728d41f305\", \"Node\": \"treeMate\", \"Address\": \"127.0.0.1\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": &#123; \"lan\": \"127.0.0.1\", \"lan_ipv4\": \"127.0.0.1\", \"wan\": \"127.0.0.1\", \"wan_ipv4\": \"127.0.0.1\" &#125;, \"Meta\": &#123; \"consul-network-segment\": \"\" &#125;, \"CreateIndex\": 10, \"ModifyIndex\": 11 &#125;] 服务端注册# pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;dependencies&gt; &lt;!--Consul client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--MVC--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--业务common类，引用当前项目的版本号--&gt; &lt;dependency&gt; &lt;groupId&gt;com.sam.cloud&lt;/groupId&gt; &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--DEV-TOOLS--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 主启动类 12345678@SpringBootApplication@EnableDiscoveryClientpublic class PaymentConsulMain8004 &#123; public static void main(String[] args) &#123; SpringApplication.run(PaymentConsulMain8004.class, args); &#125;&#125; application.yml 123456789101112server: port: 8004spring: application: name: cloud-provider-service cloud: # 启动基于consul的服务注册 consul: discovery: service-name: $&#123;spring.application.name&#125; hostname: localhost host: localhost controller只是一个简单的数据模拟 123456789101112131415161718@RestControllerpublic class PaymentConsulController &#123; @Value(\"$&#123;server.port&#125;\") private String port; /** * 返回一个随机payment即可 * @return */ @GetMapping(\"/payment/consul\") public CommonResult&lt;Payment&gt; testConsulPayment()&#123; Payment payment = new Payment(); payment.setId(RandomUtils.nextLong()); payment.setSerial(\"这是一个模拟的随机payment，\"+ RandomStringUtils.randomAlphabetic(16)); return CommonResult.success(\"I'm Consul Client on:\"+port,payment); &#125;&#125; 启动后就可以在上面的ui中看到。 客户端注册# pom application 都一毛一样，config、主启动类、controller和上面zk的一毛一样（因为暂时没用openFeign和ribbon），没啥可写的。 Eureka、Zookeeper、Consul的对比# 组件 CAP 对外接口 Eureka AP Http Consul CP Http/DNS Zookeeper CP 客户端 C主要是数据一致，Eureka主要保证高可用。 Ribbon# Ribbon简介# 是一套客户端的负载均衡工具，如链接超时、重试等，配置文件只用列出所有的节点，Ribbon自动基于规则（轮询、随机、响应时间加权等）去链接，也很容易自定义实现负载均衡。 官网在github，目前也是维护模式了。未来的趋势是Spring的LoadBalancer，但是还很不成熟。 Ribbon： 本地负载均衡，进程内。调用前从注册中心获取服务信息，缓存到JVM，本地负载均衡。 负载均衡+RestTemplate进行RPC，可以和多种客户端结合。Eureka只是其中之一。 工作时分两步： 先选择注册中心，比如先从注册中心选择一个负担小的Eureka 根据用户指定的策略，从注册地址取到一个进行。 Nginx：是服务端的LB。 新版2.2.x的springcloud的eureka会自动引入ribbon： 12345678910 &lt;!--eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--上面的eureka-client会自动引入ribbon--&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;--&gt;&lt;!-- &lt;/dependency&gt; Ribbon的负载均衡策略# 都是IRule的实现，策略模式。 RoundRobinRule： 默认轮询的方式 RandomRule： 随机方式 WeightedResponseTimeRule： 根据响应时间来分配权重的方式，响应的越快，分配的值越大。 BestAvailableRule： 选择并发量最小的方式 RetryRule： 在一个配置时间段内当选择server不成功，则一直尝试使用subRule的方式选择一个可用的server。 ZoneAvoidanceRule： 根据性能和可用性来选择。 AvailabilityFilteringRule： 过滤掉那些因为一直连接失败的被标记为circuit tripped的后端server，并过滤掉那些高并发的的后端server（active connections 超过配置的阈值） 其他负载均衡算法：# http://dubbo.apache.org/zh-cn/docs/source_code_guide/loadbalance.html LeastActiveLoadBalance：最小活跃数负载均衡算法 活跃调用越少，说明server性能越高。优先给他。具体实现：每个服务者对应一个活跃数，init的时候大家都为0，收到一个请求+1，处理完毕-1。一段时间后性能最好的机器下降速度最快，优先给他新的请求。 ConsistentHashLoadBalance：一致性hash算法 如nginx的IP hash，把client的ip或者url等进行hash，对同一个client，相同的请求永远在一台机器。 自定义规则# 注意事项：# 自定义ribbon规则类，不能放在@ComponentsScan所能扫描的包和子包内(主启动类和以下所有包)。否则这个配置类会被所有的Ribbon客户端共享，达不到特殊定制化目的。 注意@ComponentScan 和@SpringBootApplication注解都不要扫描到。 原始的轮询规则# 默认的那个轮询规则： discoveryClient拿到所有的Server实例，然后搞一个int计数，每次取模决定返回哪个Server。 里面有自旋锁、AQS，避免重量级锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private AtomicInteger nextServerCyclicCounter; // 注意这里是一个原子型的Integerpublic Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; log.warn(\"no load balancer\"); return null; &#125; Server server = null; int count = 0; while (server == null &amp;&amp; count++ &lt; 10) &#123; List&lt;Server&gt; reachableServers = lb.getReachableServers(); // 获取可用的server List&lt;Server&gt; allServers = lb.getAllServers(); //所有的server int upCount = reachableServers.size(); int serverCount = allServers.size(); if ((upCount == 0) || (serverCount == 0)) &#123; log.warn(\"No up servers available from load balancer: \" + lb); return null; &#125; int nextServerIndex = incrementAndGetModulo(serverCount); // 注意，这里调用了下面，使用了一个自旋锁。传入服务器总数量。 server = allServers.get(nextServerIndex); if (server == null) &#123; /* Transient. */ Thread.yield(); continue; &#125; if (server.isAlive() &amp;&amp; (server.isReadyToServe())) &#123; return (server); &#125; // Next. server = null; &#125; if (count &gt;= 10) &#123; log.warn(\"No available alive servers after 10 tries from load balancer: \" + lb); &#125; return server; &#125; /** * Inspired by the implementation of &#123;@link AtomicInteger#incrementAndGet()&#125;. * * @param modulo The modulo to bound the value of the counter. * @return The next value. */ private int incrementAndGetModulo(int modulo) &#123; for (;;) &#123; int current = nextServerCyclicCounter.get(); int next = (current + 1) % modulo; if (nextServerCyclicCounter.compareAndSet(current, next)) return next; &#125; &#125; 定义手动实现一个负载均衡# 步骤： ApplicationContextConfig对象上面去掉@LoadBalanced注解（restTemplate上面），否则就会使用ribbon自带的策略 写一个LoadBalanced接口 实现接口来一个choose方法，使用discoverClient去根据策略选择一个instance 在controller的请求时候注入这个负载均衡，choose一个insrance，拿到uri，拼装请求地址。 123456789101112131415161718192021222324252627282930313233@Componentpublic class MyLoadBalancer implements ICustomerLoadBalancer &#123; // 计数器 private AtomicInteger integer = new AtomicInteger(0); @Override public ServiceInstance chooseInstance(List&lt;ServiceInstance&gt; instances) &#123; if(instances == null || instances.isEmpty())&#123; System.out.println(\"没有可用的服务！！！！！\"); return null; &#125; // 计数器线程安全得+1，对server数量取模 int i = incrementAndGet() % instances.size(); return instances.get(i); &#125; /** * 自选增加，获取下一个值 * @return */ private final int incrementAndGet()&#123; int curr,next; do&#123; curr = integer.get(); next = curr &gt; Integer.MAX_VALUE ? 0 : curr+1; // int不能超限 // 只要没获取到真正的curr，就一直自旋 &#125;while (!integer.compareAndSet(curr, next)); // true之后就中断 System.out.println(\"------------------next:\"+next); return next; &#125;&#125; controller，使用上面的loadbalancer手动获取instance地址url地址 12345678910111213/** * 测试使用自定义手动负载均衡的方式获取 * @param id * @return */@GetMapping(\"/customer/payment/lb/get/&#123;id&#125;\")public CommonResult&lt;Payment&gt; getByIdByHanLoadBalancer(@PathVariable(\"id\") long id)&#123; List&lt;ServiceInstance&gt;instances = discoveryClient.getInstances(\"CLOUD-PAYMENT-SERVICE\"); // 使用我们的自定义轮循负载均衡器 ServiceInstance instance = myLoadBalancer.chooseInstance(instances); // getForObject 返回Json数据，可以转化为对象 return restTemplate.getForObject(instance.getUri()+\"/payment/get/\"+id, CommonResult.class);&#125; 动手实现一个自定义ribbon策略# 自定义策略，实现IRule接口，或者继承自AbstractLoadBalancerRule 在@SpringBootApplication扫描位置外定义一个Config，里面配置我们的自定义策略 在主启动类加上我们的自定义策略配置 RestTemplate类加上@LoadBalanced注解 controller正常请求类似 自定义Rule实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class MyRibbonRule extends AbstractLoadBalancerRule&#123; @Resource private DiscoveryClient discoveryClient; // 获取服务实例用 /** * 人工实现一个轮询策略，在主启动类使用了下面的注解引用过来 * @RibbonClient(name = \"CLOUD-PAYMENT-SERVICE\", configuration = MyRibbonRule.class) //为每个服务定制规则 */ AtomicInteger integer = new AtomicInteger(0); // 自定义核心方法，挑选主机 private Server choose(ILoadBalancer lb, Object key)&#123; if (lb == null) &#123; return null; &#125; Server server = null; while (server == null) &#123; if (Thread.interrupted()) &#123; return null; &#125; List&lt;Server&gt; upList = lb.getReachableServers(); //List&lt;Server&gt; allList = lb.getAllServers(); int i = integer.getAndIncrement(); System.out.println(\"MyRibbonRule----integer-cnt=\"+i); if(i &gt; Integer.MAX_VALUE)&#123; integer.set(0); &#125; server = upList.get(i%upList.size()); if (server == null) &#123; /* * The only time this should happen is if the server list were * somehow trimmed. This is a transient condition. Retry after * yielding. */ Thread.yield(); continue; &#125; // 选了个挂的，重来 if (server.isAlive()) &#123; return (server); &#125; // Shouldn't actually happen.. but must be transient or a bug. server = null; Thread.yield(); &#125; return server; &#125; @Override public void initWithNiwsConfig(IClientConfig iClientConfig) &#123; // 初始化 System.out.println(\"--------------init rule-------------\"); System.out.println(iClientConfig.getClientName()); System.out.println(iClientConfig.getProperties()); &#125; @Override public Server choose(Object key) &#123; // 根据一个key返回一个Server return choose(getLoadBalancer(), key); &#125;&#125; 配置实例化： 1234567891011121314151617181920@Configurationpublic class ApplicationContextConfig &#123; /** * 类似&lt;bean id=\"xxx\", class=\"xxxx\"&gt;&lt;/bean&gt; * @return */ @Bean @LoadBalanced //当调用服务方集群的时候，需要加上这个，然后restTemplate请求地址写cloud注册的服务名即可。 // 自定义负载均衡策略的时候不用这个注解 public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125; @Bean public IRule getRule()&#123; return new MyRibbonRule(); &#125; &#125; 主启动类 12345@SpringBootApplication@EnableEurekaClient@RibbonClient(name = \"CLOUD-PAYMENT-SERVICE\", configuration = MyRibbonRule.class) //为每个服务定制规则@EnableDiscoveryClient // 给自定义规则用，这里可以获取实例列表public class CustomerMyRibbonMain8083 &#123; controller： 12345678910/** * 测试使用自定义负载均衡策略的方式获取 * @param id * @return */ @GetMapping(\"/customer/payment/rule/get/&#123;id&#125;\") public CommonResult&lt;Payment&gt; getByIdByIRuleBalancer(@PathVariable(\"id\") long id)&#123; // getForObject 返回Json数据，可以转化为对象 return restTemplate.getForObject(url+\"/payment/get/\"+id, CommonResult.class); &#125; 参考链接： https://blog.csdn.net/qq_41211642/article/details/104772140#comments","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://blog.sofunnyai.com/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://blog.sofunnyai.com/tags/SpringCloud/"}]},{"title":"let's-encrypt配置网站免费SSL证书","slug":"基于lets-encrypt的https证书","date":"2019-07-13T18:44:23.000Z","updated":"2020-05-24T12:08:50.002Z","comments":true,"path":"article/lets-encrypt.html","link":"","permalink":"https://blog.sofunnyai.com/article/lets-encrypt.html","excerpt":"","text":"前言 厂商选择 下载安装cert-bot 安装cert-bot 获取证书 直接安装到nginx 只生成证书 测试自动更新证书 纯手动的certonly 参考链接 前言# 相信看到这里的对SSL/TSL都有一定了解，链式信任、防劫持、防隐私泄露、安全可信，这些关键字大家脑海里都很熟悉。具体SSL细节就不啰嗦了，感兴趣可以去看看阮一峰的博客或者网上资料，本文主要是实操。 厂商选择# 除了域名统配的高端证书之外，一般我们的博客或者小型网站可以考虑使用免费厂商提供的证书。和大厂商的主要区别就是公信力了，不过理论上都是同等安全的。比如： https://letsencrypt.org/ 在chrome等浏览器厂商的努力支持下，这些之前看起来小点的证书厂商现在兼容性也非常好了。 本文就以let's encrypt和nginx为例。 下载安装cert-bot# 废话少说，现在let's encrypt是推荐在server上使用cert-bot来安装、更新我们的证书，https://certbot.eff.org/ 所以： 安装cert-bot# 1yum install certbot python2-certbot-nginx 获取证书# 首先把我们的域名解析到当前机器的nginx上，80可以正常访问。 然后获取证书有两种方式：1.直接自动安装到nginx，并由cert-bot管理nginx配置文件。 2.获取证书，但手动修改nginx配置文件 直接安装到nginx# 配置环境变量： 123 ln -s /main/server/nginx/sbin/nginx /usr/bin/nginxln -s /main/server/nginx/conf/ /etc/nginxcertbot --nginx 会自动识别nginx配置文件，生成nginx的证书，并修改nginx文件。这是最简单的方式。 只生成证书# 只生成证书： 1certbot certonly --nginx 会让你输入邮箱、域名等信息 测试自动更新证书# 配置自动更新 1echo \"0 0,12 * * * root python -c 'import random; import time; time.sleep(random.random() * 3600)' &amp;&amp; certbot renew -q\" | sudo tee -a /etc/crontab &gt; /dev/null 测试一下： 1certbot renew -q --dry-run 如果报错一个ASCII错误问题，是因为nginx的配置文件有中文。。。所以还是建议只生成证书，手动去配置nginx比较好。 纯手动的certonly# 适用于上面的certonly报错的时候 certbot certonly --manual --email xxx@xxx.com -d *.domain.com 参考链接# https://certbot.eff.org/lets-encrypt/centosrhel7-nginx https://www.jianshu.com/p/6ea81a7b768f https://www.jianshu.com/p/a1cc68c7d916","categories":[{"name":"network","slug":"network","permalink":"https://blog.sofunnyai.com/categories/network/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://blog.sofunnyai.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/tags/hexo/"},{"name":"运维","slug":"运维","permalink":"https://blog.sofunnyai.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"linux下mysql5.7安装备忘","slug":"mysql5-7","date":"2018-10-21T03:41:55.000Z","updated":"2020-06-02T03:47:11.759Z","comments":true,"path":"article/linux-mysql5.7-install.html","link":"","permalink":"https://blog.sofunnyai.com/article/linux-mysql5.7-install.html","excerpt":"","text":"先下载 解压，安装依赖 编辑配置文件 设置环境变量 创建目录 创建用户，授权 初始化 启动，修改密码 启动 修改密码 开机自启动 服务命令 安装过多次，5.7略有变化，记录供日后参考。 先下载# 去https://dev.mysql.com/downloads/mysql/5.7.html下载，比如wget mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz 解压，安装依赖# apt install libaio1,否则会报一个libaio.so找不到的错。centos使用yum安装。 编辑配置文件# vim /etc/my.cnf然后编辑 123456789101112131415161718192021222324252627282930313233[mysql]# 设置mysql客户端默认字符集default-character-set=utf8mb4socket=/var/run/mysqld/mysqld.sock[mysqld]skip-name-resolveport = 3306#language=/usr/local/mysql/share/englishsocket=/var/run/mysqld/mysqld.sock# 设置mysql的安装目录basedir=/main/mysql5.7.30# 设置mysql数据库的数据的存放目录datadir=/main/mysql5.7.30/datalog-error=/var/log/mysql/error.logslow_query_log=ONslow_query_log_file=/var/log/mysql/slowquery.loglong_query_time=3log-queries-not-using-indexes=ON# 允许最大连接数max_connections=200# 服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8mb4# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB lower_case_table_names=1max_allowed_packet=16Muser=mysqldefault-time_zone = '+8:00'[client]port = 3306socket = /var/run/mysqld/mysqld.sock 设置环境变量# 123#mysqlexport MYSQL_HOME=/main/mysql5.7.30export PATH=$MYSQL_HOME/bin:$PATH 创建目录# 5.7很弱智，上面配置文件中的那些目录都要人肉创建，否则会报错。 123mkdir /main/mysql5.7.30/datamkdir /var/log/mysqlmkdir /var/run/mysqld 创建用户，授权# 123456groupadd mysql useradd -r -g mysql mysql chown -R mysql:mysql /main/mysql5.7.30chown -R mysql:mysql /var/log/mysqlchown -R mysql:mysql /var/run/mysqld 初始化# 1mysqld --initialize --user=mysql --basedir=/main/mysql5.7.30 --datadir=/main/mysql5.7.30/data --lc_messages_dir=/main/mysql5.7.30/share --lc_messages=en_US 然后观察控制台的信息和/var/log/mysql/error.log里面的输出，缺少啥目录或者文件手动创建一下即可。 启动，修改密码# 启动# 初始化后，去到/var/log/mysql/error.log找到里面的临时密码，然后启动： /main/mysql5.7.30/support-files/mysql.server start 正常启动后登录： mysql -u root -p然后输入刚才复制的临时密码 修改密码# 12345678910set password=password('新密码'); flush privileges; use mysql; UPDATE `mysql`.`user` SET `Host`='%', `User`='root', `Select_priv`='Y', `Insert_priv`='Y', `Update_priv`='Y', `Delete_priv`='Y', `Create_priv`='Y', `Drop_priv`='Y', `Reload_priv`='Y', `Shutdown_priv`='Y', `Process_priv`='Y', `File_priv`='Y', `Grant_priv`='Y', `References_priv`='Y', `Index_priv`='Y', `Alter_priv`='Y', `Show_db_priv`='Y', `Super_priv`='Y', `Create_tmp_table_priv`='Y', `Lock_tables_priv`='Y', `Execute_priv`='Y', `Repl_slave_priv`='Y', `Repl_client_priv`='Y', `Create_view_priv`='Y', `Show_view_priv`='Y', `Create_routine_priv`='Y', `Alter_routine_priv`='Y', `Create_user_priv`='Y', `Event_priv`='Y', `Trigger_priv`='Y', `Create_tablespace_priv`='Y', `ssl_type`='', `ssl_cipher`='', `x509_issuer`='', `x509_subject`='', `max_questions`='0', `max_updates`='0', `max_connections`='0', `max_user_connections`='0', `password_lifetime`=NULL, `account_locked`='N' WHERE (`User`='root'); flush privileges; 开机自启动# 123cd support-filescp mysql.server /etc/init.d/mysqldchkconfig --add mysqld 服务命令# 使用的support-files目录下的文件管理 123mysql.server stopmysql.server startmysql.server restart","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.sofunnyai.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"https://blog.sofunnyai.com/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"运维","slug":"运维","permalink":"https://blog.sofunnyai.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.sofunnyai.com/tags/mysql/"}]},{"title":"IO基础：socket和TCP","slug":"IO模型之selector和Epoll","date":"2018-09-12T12:12:26.000Z","updated":"2020-05-22T07:38:16.341Z","comments":true,"path":"article/io-basic-socket-tcp.html","link":"","permalink":"https://blog.sofunnyai.com/article/io-basic-socket-tcp.html","excerpt":"","text":"IO# OSI基础模型# 提到IO，首先是OSI参考模型，计算机网络基础，一共七层 这7层是一个虚的东西，是一个规范。TCP/IP协议给精简到4层，把上面的应用层-表示层-会话层统一归结到新的应用层是用户称，把下面的传输控制层-网络层-链路层-物理层视为内核层。 OSI七层网络模型 TCP/IP四层概念模型 对应网络协议 应用层（Application） 应 HTTP、TFTP, FTP, NFS, WAIS、SMTP 表示层（Presentation） 用 Telnet, Rlogin, SNMP, Gopher 会话层（Session） 层 SMTP, DNS 传输层（Transport） 传输层 TCP, UDP 网络层（Network） 网络层 IP, ICMP, ARP, RARP, AKP, UUCP 数据链路层（Data Link） 数据 FDDI, Ethernet, Arpanet, PDN, SLIP, PPP 物理层（Physical） 链路层 IEEE 802.1A, IEEE 802.2到IEEE 802.11 linux命令测试讲解TCP# 创建一个到baidu的文件描述符（内核层）# 执行一个bash命令创建一个到baidu的socket，放到当前进程的8号文件描述符中：exec 8&lt;&gt; /dev/tcp/www.baidu.com/80 linux一切皆文件，上面面创建了一个“8”文件，是一个socket指向了百度， 8是文件描述符fd(就像代码的变量)，&lt;&gt;是一个双向输入输出流，可以看到 echo $$ 16199 # 打印当前命令行的进程号 # 也可以ps -ef 然后grep出来 tree 16199 9368 0 4月15 pts/1 00:00:01 /bin/bash 123456789101112- 可以去当前进程的目录看一眼- &#96;&#96;&#96;bash cd &#x2F;proc&#x2F;16199&#x2F;fd # 进入当前进程的fd目录 ls # 看一眼 lrwx------ 1 tree tree 64 5月 21 18:31 0 -&gt; &#x2F;dev&#x2F;pts&#x2F;1 lrwx------ 1 tree tree 64 5月 21 18:31 1 -&gt; &#x2F;dev&#x2F;pts&#x2F;1 lrwx------ 1 tree tree 64 5月 21 18:31 2 -&gt; &#x2F;dev&#x2F;pts&#x2F;1 lrwx------ 1 tree tree 64 5月 21 18:31 255 -&gt; &#x2F;dev&#x2F;pts&#x2F;1 lrwx------ 1 tree tree 64 5月 21 18:31 8 -&gt; &#39;socket:[1037956]&#39; # 每个进程都有0,1,2三个fd文件描述符。分别是stdin、stdout、stderr 向文件描述符中写东西通信（用户层态）# 123456echo -e \"GET / HTTP/1.0\\n\" 1&gt;&amp; 8 # 打印一个字符串到标准输出（所以是1）重定向&gt;到文件描述符(所以是&amp;，重定向到文件不用&amp;)8中cat 0&lt;&amp; 8# 从文件描述符(所以是&amp;，文件的话不用&amp;)8中标准输入&lt;# 。。。。。下面打印一大堆百度的html 传输控制层TCP协议# 什么是socket套接字？# ip+port &lt;---------&gt; ip+port 是一【套】，客户端和服务端的ip+port 4个要素决定唯一的一个socket 客户端的ip是B，可以和baidu建立多少个链接？65535个 此时客户端B还能继续和163建立链接吗？也可以继续再次建立65535个，因为socket是【一套】4个要素，server换了就是另外一个socket了。 对于类似如下netstat -anp出来的socket链接，每一个established都有一个文件描述符(fd目录下)数字和他对应并交给一个进程。程序只用和这个文件描述符进行读写就可以进行socket通信了。【如果多个socket对应一个进程：就是多路复用器selector或者epoll】 什么是TCP协议？# 是一个面向连接的可靠的传输协议。因为三次握手保证了可靠传输。 三次握手的细节？# C-----------syn-----------&gt;S # “我要跟你连接了，标识是syn” C&lt;----------syn+ack-------S # “好的，我知道了” 让客户端知道Server已经响应了 C------------ack-----------&gt;S # 好的，我知道你知道了。让Server知道发出的消息客户端收到了 三次握手完毕后，双方才有资源开辟，才能开始传输。 tcpdump 四次分手的细节，为啥要四次？# 因为握手是三次，开辟了资源。分手释放资源是双方的，所以是四次（双方都要同时释放，不能轻易单方面释放了） 分手的C只是先说断开的人 C-----------fin-----------&gt;S # “我要跟你分手了，标识是fin”给Server一个结束标识 C&lt;----------fin+ack-------S # “好的，我知道了” 让客户端知道Server已经响应了（但是我要确认一下真的没事儿了） C&lt;-----------fin------------S # “好吧，分吧，标识是fin”确认真的没事儿了，给客户端一个结束标识 C------------ack-----------&gt;S # “好的，”让Server知道发出的消息客户端收到了 三次握手和四次分手是不可分割的最小粒度# LVS作为一个工作在四层的负载均衡，是无法知晓数据包的具体内容的！ LVS是否可以随意把数据给后端进行负载？-可以负载，但是受制于协议约束！ C ----- lvs ----- S1/S2 的时候，LVS必须要把握手的三次给到一对C—S，不能给到另外一个S，否则无法建立连接。 网络和路由# 网络设置要ip、gateway、mask、dns4个东西","categories":[{"name":"IO","slug":"IO","permalink":"https://blog.sofunnyai.com/categories/IO/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://blog.sofunnyai.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"基础","slug":"基础","permalink":"https://blog.sofunnyai.com/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"CAS比较并交换的原理和ABA问题","slug":"CAS和ABA","date":"2018-03-01T05:05:11.000Z","updated":"2020-06-01T16:31:44.597Z","comments":true,"path":"article/cas-and-aba.html","link":"","permalink":"https://blog.sofunnyai.com/article/cas-and-aba.html","excerpt":"","text":"CAS简介 Atomic修饰的类为啥是线程安全的？如AtomicInteger CAS缺点 小结 CAS CAS应用 ABA问题是什么？原子更新引用是什么？ CAS导致ABA问题 ABA问题 原子引用AtomicReference 带版本的原子引用解决ABA问题 AtomicStampedReference LongAdder（CAS机制优化） 为什么有了AtomicLong还要新增一个LongAdder呢 参考 文章主线: CAS----&gt;Unsafe----&gt;CAS底层实现和思想----&gt;ABA问题----&gt;原子引用更新----&gt;如何规避ABA问题 这篇博客还不错 https://blog.csdn.net/javazejian/article/details/72772470 CAS简介# CompareAndSet，比较并交换思想，底层是CPU并发源语言。功能是判断某个内存是为是否是预期的值，true则更新，false放弃。过程是原子的（线程安全）。 CPU原语音硬件实现，执行必须连续，所以天生就是原子的。线程安全。 底层通过Unsafe类实现，Unsafe中的CAS方法JVM会生成汇编指令。 Atomic修饰的类为啥是线程安全的？如AtomicInteger# AtomicInteger.java 的 getAndIncrement方法，获取值并+1，底层借助了JDK的rt.jar中的unsafe类的native方法。 unsafe类是sun.misc.Unsafe，使用本地方法帮Java操纵底层。如读写操纵特定内存位置的数据。----直接操纵操作系统执行任务 下面代码里直接用unsafe去操纵内存偏移位置上的对象，是不允许中断的连续的指令，是线程安全的，所以没有并发问题。 变量value使用volatile修饰，保证多线程的可见性 123456789private volatile int value; // 当前AtomicInteger内部使用了一个volatile修饰的int存储值。/** * Atomically increments by one the current value. * * @return the previous value */ public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1); // 传入当前AtomicInteger对象，在内存的地址偏移量对应对象，加1 &#125; unsafe.java他的底层实现 12345678910111213// unsafe类的 getAndAddInt 方法public final int getAndAddInt(Object var1, long var2, int var4) &#123; // var1 是上面的this对象，就是当前的那个Integer，var2是unsafe找到的内存地址，var4是要新增的值 int var5; // var5 是内存快照里面的值 do &#123; var5 = this.getIntVolatile(var1, var2); // 当前这个var1（上面this对象）在内存地址var2上去取到volatile的快照值放到var5中 &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); // var1对象在var2内存地址上的值和快照值var5比较，相同就给var5+var4，结束do...while。如果拿出来的快照值不相等，继续do...while，上面去继续 return var5; &#125;// getIntVolatile 和 compareAndSwapInt是当前unsafe类中的native方法，借助CPU源语实现线程安全。（后面有个JVM native方法的源码） var5：就是我们从主内存volatile中拷贝到工作内存中的值操作的时候，需要比较工作内存中的值，和主内存中的值进行比较 假设执行 compareAndSwapInt返回false，那么就一直执行 while方法，直到期望的值和真实值一样 val1：AtomicInteger对象本身 var2：该对象值的引用内存地址 var4：需要修改变动的数量 var5：用var1和var2找到的内存中的真实值 用该对象当前的值与var5比较 如果相同，更新var5 + var4 并返回true，结束循环。 如果不同，继续取volatile值然后再比较，直到更新完成 这里没有用synchronized，而用CAS，这样提高了并发性，也能够实现一致性，是因为每个线程进来后，进入的do while循环，然后不断的获取内存中的值，判断是否为最新，然后在进行更新操作。 假设线程A和线程B同时执行getAndAddInt操作（分别跑在不同的CPU上） AtomicInteger里面的value原始值为3，即主内存中AtomicInteger的 value 为3，根据JMM模型，线程A和线程B各自持有一份价值为3的副本，分别存储在各自的工作内存 线程A通过getIntVolatile(var1 , var2) 拿到value值3，这是线程A被挂起（该线程失去CPU执行权） 线程B也通过getIntVolatile(var1, var2)方法获取到value值也是3，此时刚好线程B没有被挂起，并执行了compareAndSwapInt方法，比较内存的值也是3，成功修改内存值为4，线程B打完收工，一切OK 这是线程A恢复，执行CAS方法，比较发现自己手里的数字3和主内存中的数字4不一致，说明该值已经被其它线程抢先一步修改过了，那么A线程本次修改失败，只能够重新读取后在来一遍了，也就是在执行do while 线程A重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总能够看到，线程A继续执行compareAndSwapInt进行比较替换，直到成功。 compareAndSwapInt的底层CPU源语实现： Unsafe类 + CAS思想： 也就是自旋，自我旋转 CAS缺点# 和synchronized比较，没有加锁每个线程都可以同时竞争并发计算，但是也有以下缺点： 循环太多的时候，自旋太多。do…while太多，CPU开销大。（没有锁，需要大量比较） 只能保证一个变量的原子性，不像synchronized可以对多个变量进行原子性操作。 引出了ABA问题，两个线程在执行CAS竞争的时候，一个线程T1很慢，另一个T2很快。初始的时候快照变量都是A，在拿到线程内存中后，T2修改为B并CAS写回成功。然后T2再来一次CAS把变量从B改为A，后来T1才回来，一看主内存还是A，CAS成功改成C。但这时候的A和刚才的A虽然值一样，但是可能业务发生了变化。造成问题。具体见下一章。 小结# CAS# CAS是compareAndSwap，比较当前工作内存中的值和主物理内存中的值，如果相同则执行规定操作，否者继续比较直到主内存和工作内存的值一致为止 CAS应用# CAS有3个操作数，内存值V，旧的预期值A，要修改的更新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否者什么都不做 ABA问题是什么？原子更新引用是什么？# 假设现在有两个线程，分别是T1 和 T2，然后T1执行某个操作的时间为10秒，T2执行某个时间的操作是2秒，最开始AB两个线程，分别从主内存中获取A值，但是因为B的执行速度更快，他先把A的值改成B，然后在修改成A，然后执行完毕，T1线程在10秒后，执行完毕，判断内存中的值为A，并且和自己预期的值一样，它就认为没有人更改了主内存中的值，就快乐的修改成B，但是实际上 可能中间经历了 ABCDEFA 这个变换，也就是中间的值经历了狸猫换太子。 所以ABA问题就是：在获取主内存值的时候，该内存值在我们写入主内存的时候，已经被修改了N次，但是最终又改成原来的值了 CAS导致ABA问题# CAS算法实现了一个重要的前提，需要取出内存中某时刻的数据，并在当下时刻比较并替换，那么这个时间差会导致数据的变化。 比如说一个线程one从内存位置V中取出A，这时候另外一个线程two也从内存中取出A，并且线程two进行了一些操作将值变成了B，然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后线程one操作成功 尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的，看业务情况。 ABA问题# 如果业务只管最终的数值是无所谓的，最终都是A即可。ABA问题就可以接受。（只看结果） 但是如果业务要求中间不能被别的线程偷偷改变还不知道（如支付类）。ABA问题就无法被接受。（要求过程） 原子引用AtomicReference# 原子引用其实和原子包装类是差不多的概念，就是将一个java类，用原子引用类进行包装起来，那么这个类就具备了原子性. 但是原子引用打来了ABA问题。 带版本的原子引用解决ABA问题 AtomicStampedReference# 通过一个版本号stamp来解决ABA问题，内同一致但是版本号不一致还是不能提交修改。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128package com.sam.phoenix.concurrent;import lombok.AllArgsConstructor;import lombok.Data;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;/** * CAS虽然可以自旋解决变量竞争的线程安全问题，但是会带来ABA问题 * * CAS算法实现了一个重要的前提：**需要取出内存中某时刻的数据，并在当下时刻比较并替换，那么这个时间差会导致数据的变化。** * 比如说一个线程one从内存位置V中取出A，这时候另外一个线程two也从内存中取出A，并且线程two进行了一些操作将值变成了B，然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后线程one操作成功 * * `尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的，看业务情况。` * * ## ABA问题 * 如果业务只管最终的数值是无所谓的，最终都是A即可。ABA问题就可以接受。（只看结果） * 但是如果业务要求中间不能被别的线程偷偷改变还不知道（如支付类）。ABA问题就无法被接受。（要求过程） */public class CAS_ABA &#123; public static void main(String[] args) &#123; System.out.println(\"--------------------以下是一个CAS引发的ABA问题的demo-------------------------\"); casABA(); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"-----------------------以下是一个ABA解决，使用带版本号的stampRefference-------------------------\"); resolveABA(); try &#123; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; /** * 演示CAS带来的ABA问题 */ private static void casABA()&#123; // 初始化原子引用 // 定义一个原子引用（包装了一个自定义类），初始化值是张三 AtomicReference&lt;User&gt; atomicUser = new AtomicReference&lt;User&gt;(); User z3 = new User(\"z3\", 33); atomicUser.set(z3); new Thread(()-&gt;&#123; // 完成一次ABA User l4 = new User(\"l4\", 44); atomicUser.compareAndSet(z3, l4); // 对比主内存，如果是张三，就修改为李四 atomicUser.compareAndSet(l4,z3); // 对比主内存，如果是李四，就修改为张三 &#125;,\"t1\").start(); new Thread(()-&gt;&#123; try &#123; // 睡眠1秒，等待上面ABA完成(上面偷梁换柱，偷偷改过一次) TimeUnit.SECONDS.sleep(1); User w5 = new User(\"w5\", 55); boolean result = atomicUser.compareAndSet(z3, w5); // 虽然更新成功，但是其实里面的z3已经被修改过一次了 System.out.println(\"最终t2\"+result+\",atomicUser中的user：\"+atomicUser.get()); System.out.println(\"虽然更新成功，但是其实里面的z3已经被修改过一次了,某些业务场景会出错!!!\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;,\"t2\").start(); &#125; /** * 延时使用版本号解决ABA问题 */ private static void resolveABA()&#123; User z3 = new User(\"z3\", 33); // 初始化一个张三进去，版本号是1 AtomicStampedReference&lt;User&gt; stampedReference = new AtomicStampedReference&lt;&gt;(z3, 1); new Thread(()-&gt;&#123; User l4 = new User(\"l4\", 44); // 期待是张三，相等的话就改成李四，同时期待版本号是1 int stamp = stampedReference.getStamp(); // 休眠1秒，等待下面的线程获取同样的版本号 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result = stampedReference.compareAndSet(z3, l4, stamp,stamp+1); System.out.println(Thread.currentThread().getName()+\"尝试更新版本号\"+stamp+\"，结果：\"+result+\",stampedReference中最新的数据：\"+stampedReference.getReference()+\"最新版本号\"+stampedReference.getStamp()); stamp = stampedReference.getStamp(); result = stampedReference.compareAndSet(l4, z3, stamp,stamp+1); System.out.println(Thread.currentThread().getName()+\"尝试更新版本号\"+stamp+\"，结果：\"+result+\",stampedReference中最新的数据：\"+stampedReference.getReference()+\"最新版本号\"+stampedReference.getStamp()); &#125;,\"t3\").start(); // 因为持有的版本号是老的，会更新失败！ new Thread(()-&gt;&#123; User w5 = new User(\"w5\", 55); int stamp = stampedReference.getStamp(); // 睡3秒，等上面完成sleep和CAS try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result = stampedReference.compareAndSet(z3, w5, stamp,stamp+1); System.out.println(Thread.currentThread().getName()+\"尝试更新版本号\"+stamp+\"，结果：\"+result+\",stampedReference中最新的数据：\"+stampedReference.getReference()+\"最新版本号\"+stampedReference.getStamp()); &#125;,\"t4\").start(); &#125;&#125;@Data@AllArgsConstructorclass User&#123; private String name; private int age;&#125; LongAdder（CAS机制优化）# LongAdder是java8为我们提供的新的类，跟AtomicLong有相同的效果。是对CAS机制的优化 1234567LongAdder：//变量声明public static LongAdder count = new LongAdder();//变量操作count.increment();//变量取值count 为什么有了AtomicLong还要新增一个LongAdder呢# 原因是：CAS底层实现是在一个死循环中不断地尝试修改目标值，直到修改成功。如果竞争不激烈的时候，修改成功率很高，否则失败率很高。在失败的时候，这些重复的原子性操作会耗费性能。（不停的自旋，进入一个无限重复的循环中） 核心思想：将热点数据分离。** 比如说它可以将AtomicLong内部的内部核心数据value分离成一个数组，每个线程访问时，通过hash等算法映射到其中一个数字进行计数，而最终的计数结果则为这个数组的求和累加，其中热点数据value会被分离成多个单元的cell，每个cell独自维护内部的值。当前对象的实际值由所有的cell累计合成，这样热点就进行了有效地分离，并提高了并行度。这相当于将AtomicLong的单点的更新压力分担到各个节点上。在低并发的时候通过对base的直接更新，可以保障和AtomicLong的性能基本一致。而在高并发的时候通过分散提高了性能。 12345678910111213public void increment() &#123; add(1L);&#125;public void add(long x) &#123; Cell[] as; long b, v; int m; Cell a; if ((as &#x3D; cells) !&#x3D; null || !casBase(b &#x3D; base, b + x)) &#123; boolean uncontended &#x3D; true; if (as &#x3D;&#x3D; null || (m &#x3D; as.length - 1) &lt; 0 || (a &#x3D; as[getProbe() &amp; m]) &#x3D;&#x3D; null || !(uncontended &#x3D; a.cas(v &#x3D; a.value, v + x))) longAccumulate(x, null, uncontended); &#125;&#125; 但是这个CAS有没有问题呢？肯定是有的。比如说大量的线程同时并发修改一个AtomicInteger，可能有很多线程会不停的自旋，进入一个无限重复的循环中。 这些线程不停地获取值，然后发起CAS操作，但是发现这个值被别人改过了，于是再次进入下一个循环，获取值，发起CAS操作又失败了，再次进入下一个循环。 在大量线程高并发更新AtomicInteger的时候，这种问题可能会比较明显，导致大量线程空循环，自旋转，性能和效率都不是特别好。 于是，当当当当，Java 8推出了一个新的类，LongAdder，他就是尝试使用分段CAS以及自动分段迁移的方式来大幅度提升多线程高并发执行CAS操作的性能！ 在LongAdder的底层实现中，首先有一个base值，刚开始多线程来不停的累加数值，都是对base进行累加的，比如刚开始累加成了base = 5。 接着如果发现并发更新的线程数量过多，在发生竞争的情况下，会有一个Cell数组用于将不同线程的操作离散到不同的节点上去 ==(会根据需要扩容，最大为CPU核）==就会开始施行分段CAS的机制，也就是内部会搞一个Cell数组，每个数组是一个数值分段。 这时，让大量的线程分别去对不同Cell内部的value值进行CAS累加操作，这样就把CAS计算压力分散到了不同的Cell分段数值中了！ 这样就可以大幅度的降低多线程并发更新同一个数值时出现的无限循环的问题，大幅度提升了多线程并发更新数值的性能和效率！ 而且他内部实现了自动分段迁移的机制，也就是如果某个Cell的value执行CAS失败了，那么就会自动去找另外一个Cell分段内的value值进行CAS操作。 这样也解决了线程空旋转、自旋不停等待执行CAS操作的问题，让一个线程过来执行CAS时可以尽快的完成这个操作。 最后，如果你要从LongAdder中获取当前累加的总值，就会把base值和所有Cell分段数值加起来返回给你。 如上图所示，LongAdder则是内部维护多个Cell变量，每个Cell里面有一个初始值为0的long型变量，在同等并发量的情况下，争夺单个变量的线程会减少，这是变相的减少了争夺共享资源的并发量，另外多个线程在争夺同一个原子变量时候， 如果失败并不是自旋CAS重试，而是尝试获取其他原子变量的锁，最后当获取当前值时候是把所有变量的值累加后再加上base的值返回的。 LongAdder维护了要给延迟初始化的原子性更新数组和一个基值变量base数组的大小保持是2的N次方大小，数组表的下标使用每个线程的hashcode值的掩码表示，数组里面的变量实体是Cell类型。 Cell 类型是Atomic的一个改进，用来减少缓存的争用，对于大多数原子操作字节填充是浪费的，因为原子操作都是无规律的分散在内存中进行的，多个原子性操作彼此之间是没有接触的，但是原子性数组元素彼此相邻存放将能经常共享缓存行，也就是伪共享。所以这在性能上是一个提升。（补充：可以看到Cell类用Contended注解修饰，这里主要是解决false sharing(伪共享的问题)，不过个人认为伪共享翻译的不是很好，或者应该是错误的共享，比如两个volatile变量被分配到了同一个缓存行，但是这两个的更新在高并发下会竞争，比如线程A去更新变量a，线程B去更新变量b，但是这两个变量被分配到了同一个缓存行，因此会造成每个线程都去争抢缓存行的所有权，例如A获取了所有权然后执行更新这时由于volatile的语义会造成其刷新到主存，但是由于变量b也被缓存到同一个缓存行，因此就会造成cache miss，这样就会造成极大的性能损失） LongAdder的add操作图 可以看到，只有从未出现过并发冲突的时候，base基数才会使用到，一旦出现了并发冲突，之后所有的操作都只针对Cell[]数组中的单元Cell。 如果Cell[]数组未初始化，会调用父类的longAccumelate去初始化Cell[]，如果Cell[]已经初始化但是冲突发生在Cell单元内，则也调用父类的longAccumelate，此时可能就需要对Cell[]扩容了。 另外由于Cells占用内存是相对比较大的，所以一开始并不创建，而是在需要时候再创建，也就是惰性加载，当一开始没有空间时候，所有的更新都是操作base变量。 如上图代码： 例如32、64位操作系统的缓存行大小不一样，因此JAVA8中就增加了一个注@sun.misc.Contended解用于解决这个问题,由JVM去插入这些变量，具体可以参考openjdk.java.net/jeps/142 ，但是通常来说对象是不规则的分配到内存中的，但是数组由于是连续的内存，因此可能会共享缓存行，因此这里加一个Contended注解以防cells数组发生伪共享的情况。 为了降低高并发下多线程对一个变量CAS争夺失败后大量线程会自旋而造成降低并发性能问题，LongAdder内部通过根据并发请求量来维护多个Cell元素(一个动态的Cell数组)来分担对单个变量进行争夺资源。 可以看到LongAdder继承自Striped64类，Striped64内部维护着三个变量，LongAdder的真实值其实就是base的值与Cell数组里面所有Cell元素值的累加，base是个基础值，默认是0，cellBusy用来实现自旋锁，当创建Cell元素或者扩容Cell数组时候用来进行线程间的同步。 在无竞争下直接更新base，类似AtomicLong高并发下，会将每个线程的操作hash到不同的cells数组中，从而将AtomicLong中更新一个value的行为优化之后，分散到多个value中 从而降低更新热点，而需要得到当前值的时候，直接 将所有cell中的value与base相加即可，但是跟AtomicLong(compare and change -&gt; xadd)的CAS不同，incrementAndGet操作及其变种可以返回更新后的值，而LongAdder返回的是void。 由于Cell相对来说比较占内存，因此这里采用懒加载的方式，在无竞争的情况下直接更新base域，在第一次发生竞争的时候(CAS失败)就会创建一个大小为2的cells数组，每次扩容都是加倍，只到达到CPU核数。同时我们知道扩容数组等行为需要只能有一个线程同时执行，因此需要一个锁，这里通过CAS更新cellsBusy来实现一个简单的spin lock。 数组访问索引是通过Thread里的threadLocalRandomProbe域取模实现的，这个域是ThreadLocalRandom更新的，cells的数组大小被限制为CPU的核数，因为即使有超过核数个线程去更新，但是每个线程也只会和一个CPU绑定，更新的时候顶多会有cpu核数个线程，因此我们只需要通过hash将不同线程的更新行为离散到不同的slot即可。 我们知道线程、线程池会被关闭或销毁，这个时候可能这个线程之前占用的slot就会变成没人用的，但我们也不能清除掉，因为一般web应用都是长时间运行的，线程通常也会动态创建、销毁，很可能一段时间后又会被其他线程占用，而对于短时间运行的，例如单元测试，清除掉有啥意义呢？ 参考# AtomicLong与LongAdder（CAS机制的优化） 大白话聊聊Java并发面试问题之Java 8如何优化CAS性能？ https://blog.csdn.net/wolf_love666/article/details/87693771 https://gitee.com/moxi159753/LearningNotes","categories":[{"name":"多线程","slug":"多线程","permalink":"https://blog.sofunnyai.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"https://blog.sofunnyai.com/tags/jdk/"},{"name":"源码","slug":"源码","permalink":"https://blog.sofunnyai.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"多线程","slug":"多线程","permalink":"https://blog.sofunnyai.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"并发","slug":"并发","permalink":"https://blog.sofunnyai.com/tags/%E5%B9%B6%E5%8F%91/"}]},{"title":"深入理解woe和iv","slug":"深入理解woe和iv指标","date":"2018-01-14T03:10:14.000Z","updated":"2020-05-27T11:02:12.351Z","comments":true,"path":"article/woe-iv.html","link":"","permalink":"https://blog.sofunnyai.com/article/woe-iv.html","excerpt":"","text":"WOE(Weight of Evidence) 证据权重 WOE的定义 Woe公式理解 WOE回顾： IV值：可以认为是WOE的加权 计算woe和IV的步骤 计算注意点 WOE和IV的比较----为什么不用WOE，而是用IV值 通用WOE计算实现 WOE(Weight of Evidence) 证据权重# https://blog.csdn.net/kevin7658/article/details/50780391 https://zhuanlan.zhihu.com/p/80134853 https://www.cnblogs.com/hanxiaosheng/p/9831838.html https://www.cnblogs.com/hanxiaosheng/p/9831964.html https://blog.csdn.net/PbGc396Dwxjb77F2je/article/details/99687952 WOE的定义# WOE是一种对原始自变量进行编码的格式，可以屏蔽极值增强鲁棒性。（树模型一般只对离散变量进行编码，对极值不敏感） 要对一个变量进行WOE编码，需要首先把这个变量进行分组处理/离散化处理（等宽切割，等频切割，卡方分箱，或者利用决策树来切割）。 分组后，对于第i组，WOE的计算公式如下： woei=lnpyipni=lnpy1py0=ln(BadiBad/GoodiGood)=ln(BadiBad)−ln(GoodiGood)woe_i = ln\\frac{p_{yi}}{p_{ni}} = ln\\frac{p_{y1}}{p_{y0}} = ln(\\frac{Bad_i}{Bad}/\\frac{Good_i}{Good}) = ln(\\frac{Bad_i}{Bad})-ln(\\frac{Good_i}{Good}) woe​i​​=ln​p​ni​​​​p​yi​​​​=ln​p​y0​​​​p​y1​​​​=ln(​Bad​​Bad​i​​​​/​Good​​Good​i​​​​)=ln(​Bad​​Bad​i​​​​)−ln(​Good​​Good​i​​​​) 其中：pyi为坏样本占所有坏样本的比例，py0好样本占所有好样本的比例； Bad为坏样本总数，Badi为变量i对应的坏样本个数，Good为好样本总数，Goodi为变量i对应的好样本个数 ； 将模型目标变量y为1记为违约用户（坏样本），对于目标变量为0记为正常用户（好样本） Woe公式理解# 基础模式 woei=ln(BadiBad)−ln(GoodiGood) woe_i = ln(\\frac{Bad_i}{Bad})-ln(\\frac{Good_i}{Good}) woe​i​​=ln(​Bad​​Bad​i​​​​)−ln(​Good​​Good​i​​​​) 即 WOE = ln (第i个分箱的坏人数 / 总坏人数) - ln (第i个分箱的好人数 / 总好人数) 此时可以理解为：每个分箱里的坏人(响应)分布相对于好人(未响应)分布之间的差异性。 变换模式 woei=ln(BadiGoodi)−ln(BadGood)woe_i = ln(\\frac{Bad_i}{Good_i})-ln(\\frac{Bad}{Good}) woe​i​​=ln(​Good​i​​​​Bad​i​​​​)−ln(​Good​​Bad​​) WOE = ln (第i个分箱的坏人数 / 第i个分箱的好人数) - ln (总坏人数 / 总好人数) 此时可以理解为：每个分箱里的坏好比(Odds)相对于总体的坏好比之间的差异性。 WOE回顾：# 当前分组中，差异越大，响应的比例越大，WOE值越大； 反应的是特征的重要性，woe的绝对值越大，说明越重要。 当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。 WOE的取值范围是全体实数。(所以就不方便，需要IV缩放) WOE其实描述了变量当前这个分组，对判断个体是否会响应（或者说属于哪个类）所起到影响方向和大小，当WOE为正时，变量当前取值对判断个体是否会响应起到的正向的影响，当WOE为负时，起到了负向影响。而WOE值的大小，则是这个影响的大小的体现。 做完woe之后，LR系数不再代表特征的重要程度。 woe后LR的时候要保证系数全都是正数！ woe的符号代表特征对模型贡献的方向，系数如果不是正数就会改变这个方向。 但是做BiVar的时候已经分析了这个woe特征的贡献方向，如果LR再负数会扭曲推翻之前BiVar的分析。 优点：数值型转化为WOE可以增强鲁棒性，屏蔽极值的影响（极小值和极大值也被分组了） 但是树模型对极值不敏感，只用处理字符型即可 note：如果特征做了WOE，那么LR的系数不能代表特征重要性权重。（WOE绝对值大小已经是特征重要性了，LR的系数仅仅是拟合系数而已） woe后**如果是LR的时候要保证系数全都是正数！ **woe的符号代表特征对模型贡献的方向，系数如果不是正数就会改变这个方向。 但是做BiVar的时候已经分析了这个woe特征的贡献方向，如果LR再负数会扭曲推翻之前BiVar的分析。 核心——分箱逻辑： 实现WOE最重要的是分箱逻辑，不同的分箱会带来不同的WOE。金融常使用“基于负样本占比差异最大化”原则来分箱 一般是5箱内最好，通常最多不超过10箱 每一箱的负样本占比差值尽可能大（箱合并原则） 每一箱的样本量不少于总体5%（不要太小，不要小于三五百个样本） 通过控制划分后的总箱数，来迭代进行分箱合并 IV值：可以认为是WOE的加权# 某个分箱的IV值： IVi=(pyi−pni)∗WOEi=(BadiBadt−GoodiGoodt)∗WOEi=(BadiBadt−GoodiGoodt)∗ln(BadiBadt/GoodiGoodt)IV_i =(p_{yi}-p_{ni}) * WOE_i= (\\frac{Bad_i}{Bad_t}-\\frac{Good_i}{Good_t}) * WOE_i = (\\frac{Bad_i}{Bad_t}-\\frac{Good_i}{Good_t}) * ln(\\frac{Bad_i}{Bad_t}/\\frac{Good_i}{Good_t}) IV​i​​=(p​yi​​−p​ni​​)∗WOE​i​​=(​Bad​t​​​​Bad​i​​​​−​Good​t​​​​Good​i​​​​)∗WOE​i​​=(​Bad​t​​​​Bad​i​​​​−​Good​t​​​​Good​i​​​​)∗ln(​Bad​t​​​​Bad​i​​​​/​Good​t​​​​Good​i​​​​) 有了一个变量各分组的IV值，我们就可以计算整个变量的IV值： IV=∑inIViIV = \\sum_i^n{IV_i} IV=​i​∑​n​​IV​i​​ n是分箱的数量 对于变量的一个分组，这个分组的响应和未响应的比例与样本整体响应和未响应的比例相差越大，IV值越大，否则，IV值越小； 极端情况下，当前分组的响应和未响应的比例和样本整体的响应和未响应的比例相等时，IV值为0； IV值的取值范围是[0,+∞) ，且，当当前分组中只包含响应客户或者未响应客户时，IV = +∞。 故可以计算多个特征的IV值，按照从大到小排序来决定采用哪些特征更容易响应。（类似信息增益或者基尼指数的感觉） IV比如要大于0.05才比较好用 谨慎的时候会要求IV大于0.02就可以先留着，也就是说IV在0.02-0.5之间 超过0.5的特征会被直接拿去作为策略-------------&gt;IV太大的值可能会把模型其他特征的信息覆盖掉，也可能会造成过拟合。（如果这个特征以后抖动，造成线上效果波动） 计算woe和IV的步骤# step 1. 对于连续型变量，进行分箱（binning），可以选择等频、等距，或者自定义间隔；对于离散型变量，如果分箱太多，则进行分箱合并。 step 2. 统计每个分箱里的好人数(bin_goods)和坏人数(bin_bads)。 step 3. 分别除以总的好人数(total_goods)和坏人数(total_bads)，得到每个分箱内的边际好人占比(margin_good_rate)和边际坏人占比(margin_bad_rate)。 step 4. 计算每个分箱里的WOE [公式] step 5. 检查每个分箱（除null分箱外）里woe值是否满足单调性（bivar），若不满足，返回step1。注意⚠️：null分箱由于有明确的业务解释，因此不需要考虑满足单调性。 step 6. 计算每个分箱里的IV，最终求和，即得到最终的IV。 备注：好人 = 正常用户，坏人 = 逾期用户 计算注意点# 分箱时需要注意样本量充足，保证统计意义。 若相邻分箱的WOE值相同(非常相近)，则将其合并为一个分箱。 当一个分箱内只有好人或坏人时（会出现∞），可对WOE公式进行修正如下： Woei=ln(Badi+0.5Badt+0.5/GoodiGoodt)Woe_i = ln(\\frac{Bad_i+0.5}{Bad_t+0.5}/\\frac{Good_i}{Good_t}) Woe​i​​=ln(​Bad​t​​+0.5​​Bad​i​​+0.5​​/​Good​t​​​​Good​i​​​​) 在实践中，我们还需跨数据集检验WOE分箱的单调性。如果在训练集上保持单调，但在验证集和测试集上发生翻转而不单调，那么说明分箱并不合理，需要再次调整。（BIVAR） 或者当分箱中只有好人或坏人的时候，也可以这么做： 如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件；（即不允许这种分箱存在） 重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。 如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为0，可以人工调整响应数为1，如果非响应数原本为0，可以人工调整非响应数为1.（或者按照上面进行修正，分子分母都加0.5） WOE和IV的比较----为什么不用WOE，而是用IV值# 变量各分组的WOE和IV都隐含着这个分组对目标变量的预测能力这样的意义，但是有以下问题： 1. 各个组的WOE有正有负 解释： 假设构造一个$ WOE=\\sum_i^n{WOE_i} ，那么因为里面的WOE_i$有正有负，所以求和不好表征。 2.每个组的WOE没有考虑到这个各个组在总体的占比 解释： 即使构造一个WOE=∑in∣WOEi∣WOE=\\sum_i^n{|WOE_i|}WOE=∑​i​n​​∣WOE​i​​∣规避上面的负数问题，但是每个组WOEiWOE_iWOE​i​​的信息含量（泛化能力？）是不相同的，比如某个组WOEiWOE_iWOE​i​​很高但是这个组只有很少的样本，把他直接和另外一个很多样本但很低的WOEjWOE_jWOE​j​​相加是很不合适的。 假设某特征A分两组，从这个表我们可以看到，变量取1时，响应比达到90%，对应的WOE很高，但对应的IV却很低，原因就在于IV在WOE的前面乘以了一个系数(pyi−pni)(p_{yi}-p_{ni})(p​yi​​−p​ni​​) 而这个系数很好的考虑了这个分组中样本占整体样本的比例，比例越低，这个分组对变量整体预测能力的贡献越低。 相反，如果直接用WOE的绝对值加和，会得到一个很高的指标，这是不合理的。 通用WOE计算实现# 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# -*- coding: utf-8 -*-import mathimport pandas as pdimport numpy as npfrom pandas import DataFramefrom pandas.core.dtypes import dtypesfrom pandas.core.dtypes.common import is_numeric_dtypefrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeRegressor# 自定义实现的离散型变量woeclass charWoe(object): def __init__(self, datases: dict, dep, weight, vars: list): # 数据字典&#123;'dev':xxx,'val':xxx,'off':xxx&#125; 训练集，测试集，时间外样本集 3个dataframe self.datases = datases self.devf = datases.get('dev', '') self.valf = datases.get('val', '') self.offf = datases.get('off', '') self.dep = dep # 标签 self.weight = weight # 样本权重 self.vars = vars # 特征名 self.nrows, self.ncols = self.devf.shape # 样本数，特征数 def char_woe(self): # 得到每一类样本的个数，且加入平滑项是的bad和good都不为0 dic = dict(self.devf.groupby(self.dep).size()) # 根据标签去group，变成&#123;1:xxx,0:yyy&#125;字典 good = dic.get(0, 0) + 1e-10 # 平滑防止组内为0，计算失败 bad = dic.get(1, 0) + 1e-10 # 对每一个特征进行处理 for col in self.vars: # df[[sex,bad]].groupby(['sex','bad']).size() 会得到一个series， # 直接转成字典：&#123;(男, 0): 10553, (男, 1): 518, (女, 0): 233, (女, 1): 3&#125; # key的第一个代表特征值，第二个代表标签值 data = dict(self.devf[[col, self.dep]].groupby([col, self.dep]).size()) ''' 特征值+分类组合超过100的时候，跳过当前取值 假设二分类，dep是0,、1，则这个特征只能有50个特征值 &#123;(col特征值A,0):25,(col特征值A,1):10,(col特征值B,0):33,(col特征值B,1):21...&#125; 因为特征值过多时，WOE分箱效率低，建议进行特征截断 出现频率过低的特征就统一赋值，放到同一个箱里 ''' if len(data) &gt; 100: print(col, '有太多的特征值，建议手动进行特征截断，即将跳过此特征...') continue # 打印特征取取值个数 print('特征【%s】的取值个数是【%d】' % (col, len(data))) dic = dict() # &#123;(男, 0): 10553, (男, 1): 518, (女, 0): 233, (女, 1): 3&#125; # key的第一个代表特征值，第二个代表标签值 for (k, v) in data.items(): fea_value, dp = k # 拿出key中的特征值和标签(fea_value=男，dp=0，v=10553) dic.setdefault(fea_value, &#123;&#125;) # 给对应key设置为一个空字典（如果没有找到的话，找到的话说明之前已经设置过了） #&#123;(男, 0): 10553, (男, 1): 518&#125; ==&gt; &#123;男:&#123;1 = 518，0 = 10553&#125; , 女:&#123;...&#125; &#125; dic[fea_value][int(dp)] = v # 字典中嵌套字典 for(k, v) in dic.items(): # 计算cnt和badrate # 循环上面的嵌套字典，k=男，v=&#123;1 = xxx，0 = yyy&#125;。 # 拿出内部嵌套的字典k1 = 1 v1=xxx,生成---&gt;：&#123;‘男’：&#123; '0': 10553, '1': 518&#125;&#125; dic[k] = &#123;str(int(k1)):v1 for (k1, v1) in v.items()&#125; # 所有正负样本的和v.values(): [10553,518] dic[k]['cnt'] = sum(v.values()) # 4舍5入求bad_rate bad_rate = round(v.get(1,0)/dic[k]['cnt'], 5) dic[k][\"bad_rate\"] = bad_rate # 利用定义的函数进行合并分箱。 dic=&#123;'男': &#123;'cnt': xxx, '0': yy, '1': zz, 'bad_rate': 0.xx&#125;, 'B': &#123;'cnt': xxx, '0': yyy, '1': zz, 'bad_rate': 0.zz&#125;&#125; dic = self.combine_box_char(dic) # 对每个特征计算WOE和IV值 for (k,v) in dic.items(): a = v.get('0', 1) / good+1e-10 b = v.get('1', 1) / bad+1e-10 dic[k]['Good'] = v.get('0',0) dic[k]['Bad'] = v.get('1',0) # 下面两个是 a/b 还是 b/a？ 按照定义应该是ln(pi/pn) = ln(p_bad/p_good)? dic[k]['woe'] = round(math.log(b/a),5) dic[k]['iv'] = round((b-a)*dic[k]['woe'],5) ''' 按照分箱后的点进行分割， 计算得到每一个特征值的WOE值， 将原始特征名加上'_woe'后缀，并赋予WOE值。 ''' for (klis, v) in dic.items(): # 把分箱合并后的key切开 for k in str(klis).split(','): # 数字类型处理一下 if is_numeric_dtype(self.devf[col]): k = float(k) if '.' in k else int(k) # 训练集进行替换 self.devf.loc[self.devf[col] == k, \"%s_woe\" % col] = v[\"woe\"] self.devf.loc[self.devf[col] == k, \"%s_iv\" % col] = v[\"iv\"] # 测试集进行替换 if not isinstance(self.valf, str): self.valf.loc[self.valf[col] == k,\"%s_woe\" % col] = v[\"woe\"] self.valf.loc[self.valf[col] == k, \"%s_iv\" % col] = v[\"iv\"] # 跨时间验证集进行替换 if not isinstance(self.offf, str): self.offf.loc[self.offf[col] == k,\"%s_woe\" % col] = v[\"woe\"] self.offf.loc[self.offf[col] == k, \"%s_iv\" % col] = v[\"iv\"] # 返回新的字典，其中包含三个数据集。 return &#123;\"dev\": self.devf, \"val\": self.valf, \"off\": self.offf&#125; def combine_box_char(self, dic): ''' 实施两种分箱策略（规则）： 1.不同箱之间负样本占比(bad_rate)差异最大化。----各个特征值按照badrate从小到大排序，分别用后面一个减去前面每一个，计算badrate差值。找到差值最小的两箱合并之 2.每一箱的样本量不能过少。----当有某箱样本小于总样本的0.05，或总箱数&gt;5的时候，还是按照badrate差异最大化原则：按badrate排序后，把最小的一箱和前后比较，与差值较小的一箱合并 :param dic: 等待分箱的数据 :return: ''' # 首先合并至10箱以内。按照每一箱负样本占比差异最大化原则进行分箱。----各个特征值按照badrate从小到大排序，分别用后面一个减去前面每一个，计算badrate差值。找到差值最小的两箱合并之 while len(dic) &gt;= 10: # 拿出所有的特征和badrate，k是特征值，v['bad_rate']是负样本占比 bad_rate_dic = &#123;k:v['bad_rate'] for (k,v) in dic.items()&#125; # 按照负样本占比排序。因为离散型变量是无序的（比如学历、渠道类型） # 可以直接写成负样本占比递增的形式。(所有的dict按照value升序排序) # 得到一堆tuple的list，是(特征值，bad_rate)的一个list bad_rate_sorted = sorted(bad_rate_dic.items(), key=lambda x: x[1]) # 计算每两箱之间的负样本占比差值。 bad_rate_diff = [bad_rate_sorted[i + 1][1] - bad_rate_sorted[i][1] for i in range(len(bad_rate_sorted) - 1)] # 找到差值最小的那个，准备将其进行合并。 min_diff_index = bad_rate_diff.index(min(bad_rate_diff)) # 找到k1和k2，即差值最小的两箱的key. k1, k2 = bad_rate_sorted[min_diff_index][0], bad_rate_sorted[min_diff_index + 1][0] # 得到重新划分后的字典，箱的个数比之前少一 直接改了dic，给他里面插入一个新的分箱。key是两个key的组合！ dic[\"%s,%s\" % (k1, k2)] = dict() # 重新统计新箱的正负样本数（合并两个key的） dic[\"%s,%s\" % (k1, k2)][\"0\"] = dic[k1].get(\"0\", 0) + dic[k2].get(\"0\", 0) dic[\"%s,%s\" % (k1, k2)][\"1\"] = dic[k1].get(\"1\", 0) + dic[k2].get(\"1\", 0) # 重新统计新箱的cnt dic[\"%s,%s\" % (k1, k2)][\"cnt\"] = dic[k1][\"cnt\"] + dic[k2][\"cnt\"] # 重新计算新分箱的bad_rate dic[\"%s,%s\" % (k1, k2)][\"bad_rate\"] = round(dic[\"%s,%s\" % (k1, k2)][\"1\"] / dic[\"%s,%s\" % (k1, k2)][\"cnt\"],5) # 删除之前两个老的分箱 del dic[k1], dic[k2] ''' 结束循环后，箱的个数应该少于10。 下面实施第二种分箱策略规则：每个分箱的样本不能太少！ 将样本数量少的箱合并至其他箱中，以保证每一箱的样本数量不要太少。 ''' # 找出最少样本数的分箱 min_cnt = min([v['cnt'] for (k, v) in dic.items()]) # 当样本数量小于总样本的5%或者总箱的个数大于5的时候，对箱进行合并 【这里的5% 和 5是经验值】 while min_cnt &lt; self.nrows*0.05 or len(dic) &gt; 5: # 可能找到多个符合min_cnt的list，取第一个 min_key = [k for (k,v) in dic.items() if v['cnt'] == min_cnt][0] bad_rate_dic = &#123;k:v['bad_rate'] for (k,v) in dic.items()&#125; # 根据bad_rate升序 bad_rate_sorted = sorted(bad_rate_dic.items(),key=lambda x:x[1]) keys = [item[0] for item in bad_rate_sorted] min_key_index = keys.index(min_key) ''' 不能直接把样本数最小的两个分箱合并，因为 同样想保持合并后箱之间的负样本占比差异最大化。 由于箱的位置不同，按照三种不同情况进行分类讨论。 ''' # 如果是第一箱、第二箱 if min_key_index == 0: k1,k2 = keys[:2] elif min_key_index == len(keys)-1: # 如果是最后一箱，和倒数第二箱合并 k1,k2 = keys[-2:] else: # 如果是中间箱，前后相比和bad_rate值相差最小的箱合并 # 和前面的比 bef_bad_rate = dic[min_key]['bad_rate'] - dic[keys[min_key_index-1]]['bad_rate'] # 后面的当前比（keys是按照bad_rate升序的，不减出负数） aft_bad_rate = dic[keys[min_key_index+1]]['bad_rate'] - dic[min_key]['bad_rate'] if bef_bad_rate &lt;= aft_bad_rate: k1,k2 = keys[min_key_index-1], min_key else: k1,k2 = min_key, keys[min_key_index+1] # 找到k1，k2后合并之，同上 # 新增一个合并后的分箱 dic[\"%s,%s\" % (k1, k2)] = dict() # 重新计算cnt，bad_rate，正负样本数 dic[\"%s,%s\" % (k1, k2)][\"0\"] = dic[k1].get(\"0\", 0) + dic[k2].get(\"0\", 0) dic[\"%s,%s\" % (k1, k2)][\"1\"] = dic[k1].get(\"1\", 0) + dic[k2].get(\"1\", 0) dic[\"%s,%s\" % (k1, k2)][\"cnt\"] = dic[k1][\"cnt\"] + dic[k2][\"cnt\"] dic[\"%s,%s\" % (k1, k2)][\"bad_rate\"] = round(dic[\"%s,%s\" % (k1, k2)][\"1\"] /dic[\"%s,%s\" % (k1, k2)][\"cnt\"], 5) # 删除旧的分箱 del dic[k1], dic[k2] # 重新计算当前最小的箱的样本个数，进入下次循环继续合并分箱 min_cnt = min([v[\"cnt\"] for v in dic.values()]) return dic","categories":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/categories/risk/"}],"tags":[{"name":"评分卡","slug":"评分卡","permalink":"https://blog.sofunnyai.com/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"},{"name":"特征工程","slug":"特征工程","permalink":"https://blog.sofunnyai.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"模型评价指标KS和PSI","slug":"风控模型指标ks和psi","date":"2018-01-12T05:21:10.000Z","updated":"2020-05-25T11:57:12.481Z","comments":true,"path":"article/ks_psi.html","link":"","permalink":"https://blog.sofunnyai.com/article/ks_psi.html","excerpt":"","text":"概述 ROC曲线 回顾混淆矩阵的TPR(True Positive Rate)和FPR(False Positive Rate) ROC曲线 ROC曲线理解 KS指标 KS值定义 模型的KS值 KS(Kolmogorov-Smirnov)计算步骤： KS和ROC的区别 模型评价时: PSI群体稳定性指标 PSI(Population Stability Index)的定义 计算举例： toad底层的PSI实现 Gini指数 捕获率capture rate 减少过拟合方法： 概述# 风控指标千千万，三句话概括版本： Confusion Matrix -&gt; Lift，Gain，ROC。 ROC -&gt; AUC，KS -&gt; GINI。 MSE独立出来。 ROC曲线# 回顾混淆矩阵的TPR(True Positive Rate)和FPR(False Positive Rate)# 混淆矩阵，横着的P、N是预测结果阳性还是阴性。竖着的是说预测是否正确。 Positive Negtive T TP TN F FP FN TP：预测为正向（P），实际上预测正确（T），即判断为正向的正确率 TN：预测为负向（N），实际上预测正确（T），即判断为负向的正确率 FP：预测为正向（P），实际上预测错误（F），误报率，即把负向判断成了正向 FN：预测为负向（N），实际上预测错误（F），漏报率，即把正向判断称了负向 准确率Accuracy=（TP+TN） / （TP+FP+TN+FN）， 即预测正确的比上全部的数据 精确率、查准率 Precision=TP / （TP+FP），即在预测为正向的数据中，有多少预测正确了 召回率、查全率 Recall=TP / （TP+FN），即在所有正向的数据中，有多少预测出来了 TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率。(就是召回率，正样本被召回的比例) 【金融里面，分母是所有的good_cnt】 Recall=TPR=TPTP+FNRecall = TPR = \\frac{TP}{TP+FN} Recall=TPR=​TP+FN​​TP​​ FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率。（就是漏掉的阴性或判断错的阳性，占总阴性的比例) 【金融里面，分母是所有的bad_cnt】 FPR=FPFP+TNFPR = \\frac{FP}{FP + TN} FPR=​FP+TN​​FP​​ 更多关于TPR： /02_ml/01_THEORY/00_theory.ipynb#TPR(True-Positive-Rate) ROC曲线# 在一个二分类模型中，假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0)，阈值最小时，对应坐标点(1,1)。 如下面这幅图，(a)图中实线为ROC曲线，线上每个点对应一个阈值。纵坐标TPR，横坐标FPR。 (a) 理想情况下，TPR应该接近1，FPR应该接近0。ROC曲线上的每一个点对应于一个threshold，对于一个分类器，每个threshold下会有一个TPR和FPR。比如Threshold最大时，TP=FP=0，对应于原点；Threshold最小时，TN=FN=1，对应于右上角的点(1,1)。 (b) P和N得分不作为特征间距离d的一个函数，随着阈值theta增加，TP和FP都增加。 横轴FPR：1-TNR，1-Specificity，FPR越大，预测正类中实际负类越多。 纵轴TPR：Sensitivity(正类覆盖率)，TPR越大，预测正类中实际正类越多。 理想目标：TPR=1，FPR=0，即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。 ROC曲线理解# 主要是关于阈值的理解 roc_curve里面有一个阈值数组，里面的原理： 先按照概率值逆序排序 计算排序后所有前后之间的概率差 得到的差值数组里面有很多为0的（排序后前后概率相同），丢掉 源码里还根据二阶导数判断拐点 剩下的作为阈值数组 以上面的阈值数组下标为界，数所有label为1,0的样本【累计值】(cumsum)为tps和fps数组（有权加权） 数组tps除以最后一个tps[-1]得到tpr，fpr同理。（这里并没有判断概率值是否大于或者小于0.5，因为排序后此处这个概率值就是阈值，概率值大于这个阈值的里面，所有的+就是TP，所有的-就是FP。对应tps[i]和fps[i]） 这样计算出来的tpr 、fpr数组比原始样本数少很多（因为阈值的原因），具体理解如下 如果大家对二值分类模型熟悉的话，都会知道其输出一般都是预测样本为正例的概率，而事实上，ROC曲线正是通过不断移动分类器的“阈值”来生成曲线上的一组关键点的。可能这样讲有点抽象，还是举刚才雷达兵的例子。每一个雷达兵用的都是同一台雷达返回的结果，但是每一个雷达兵内心对其属于敌军轰炸机的判断是不一样的，可能1号兵解析后认为结果大于0.9，就是轰炸机，2号兵解析后认为结果大于0.85，就是轰炸机，依次类推，每一个雷达兵内心都有自己的一个判断标准（也即对应分类器的不同“阈值”），这样针对每一个雷达兵（样本输出），都能计算出一个ROC曲线上的关键点（一组FPR,TPR值），把大家的点连起来，也就是最早的ROC曲线了。 为方便大家进一步理解，本菇也在网上找到了一个示例跟大家一起分享【4】。下图是一个二分模型真实的输出结果，一共有20个样本，输出的概率就是模型判定其为正例的概率，第二列是样本的真实标签。 现在我们指定一个阈值为0.9，那么只有第一个样本（0.9）会被归类为正例，而其他所有样本都会被归为负例 因此，对于0.9这个阈值，我们可以计算出FPR为0，TPR为0.1（因为总共10个正样本，预测正确的个数为1），那么我们就知道曲线上必有一个点为(0, 0.1)。 依次选择不同的阈值（或称为“截断点”），画出全部的关键点以后，再连接关键点即可最终得到ROC曲线如下图所示。 sklearn中的绘制原理：（SolveKS和roc_curve代码中）# 其实还有一种更直观的绘制ROC曲线的方法，这边简单提一下。： 就是把横轴的刻度间隔设为1/N，纵轴的刻度间隔设为1/P，N和P分别为负样本与正样本数量。 然后再根据模型的输出结果【概率降序】排列， 依次遍历样本，从0开始绘制ROC曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线， 每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，遍历完所有样本点以后，曲线也就绘制完成了。 究其根本，其最大的好处便是不需要再去指定阈值寻求关键点了，每一个样本的输出概率都算是一个阈值了。当然，无论是工业界还是学术界的实现，都不可能手动去绘制，下面就来讲一下如何用Python高效绘制ROC曲线。 KS指标# KS值定义# 模型的KS值# 最理想的模型，是TPR尽量高而FPR尽量低（召回尽可能多的坏人，漏掉尽可能少的好人），然而任何模型在提高正确预测概率的同时，也会难以避免地增加误判率。 我们训练出来的模型，一般不是直接给出是正类还是负类的结果，给的是为正类的概率，我们还需要选择一个阈值，实例通过模型得到的概率大于阈值，判断为正类，小于阈值判断为负类。也就是说阈值的不同，以上的各个指标的值也是不同的。每一个阈值对应一对TPR和FPR。把阈值看成自变量，以上TPR、和FPR看成因变量，在二维坐标系里面做∣FPT−FPR∣|FPT-FPR|∣FPT−FPR∣关系曲线，这就是KS曲线。 KS曲线实操的时候是可以把将概率的阈值从小到大进行排序，取10%的值为间隔， 同理将10%*k(k=1,…9)处值作为阈值，计算不同的FPR和TPR， 以10%*k（k=1,…9）为横坐标，同时分别以TPR和FPR为纵坐标画出两条曲线就是KS曲线。 KS值是KS曲线的最大值，也就是TPR和FPR差异的最大点 KS值=max(|TPR-FPR|) KS值是在模型中用于区分预测正负样本分隔程度的评价指标。 需要计算每一箱的KS，然后max是在所有分箱的KS上取最大值 一般来说，KS大比较好。但是也不是越大越好，尤其征信行业 业内认为AUC更能体现模型的【整体的】区分能力，但是KS关注的是区分能力的最大值。 我们做的是拒绝模型，关注的是最大值的点取在哪里的。会做一个截断，小于这个值的都拒绝了。关注在最大值之前误杀了多少人。（相比于AUC注重局部而不是全局） KS(Kolmogorov-Smirnov)计算步骤：# KS用于模型风险区分能力进行评估，指标衡量的是好坏样本累计分部之间的差值。 好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。 KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。 KS值：是和AUC强相关的，但是样本很小的时候KS大，AUC不一定大。 A卡的KS： 714那种最差的一般也至少要25%， 正常的p2p公司，客户质量稍微好一点会到30%-40%左右。 最好那些有场景的分期产品最多也就不到50%，所以一般是在25%-50%之间。 B卡的KS： 至少也有40%，最高80% 一般60%左右。 各个数据集如dev和oft的KS差值不要太大，否则模型不稳定，跨时间稳定性差。 一般dev和oft的KS差值只能在5%以内，比较求稳的公司要求在3%以内。 正负样本： 逾期样本/正常样本=1%-5%（也有能做到1/1000的） 5%个点就是比较高的了，是很不均衡的。 要是坏账5%，会亏很多钱。badrte 3%以下才可能赚钱。 欺诈样本/正常样本=1/10w 欺诈用户是极少的 https://www.zhihu.com/question/37405102/answer/106668941 KS和ROC的区别# KS值对模型的评价不受样本不均衡问题的干扰，但仅限于模型评价。 模型评价时:# ROC曲线# 描绘的是不同的截断点（判断好人坏人的阈值）时，以FPR和TPR为横纵坐标轴，描述随着截断点的变化，TPR随着FPR的变化。 纵轴：TPR=正例分对的概率 = TP/(TP+FN)，其实就是查全率 横轴：FPR=负例分错的概率 = FP/(FP+TN) 作图步骤： 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）-----这就是截断点依次选取的顺序 按顺序选取截断点，并计算TPR和FPR—也可以只选取n个截断点，分别在1/n，2/n，3/n等位置 连接所有的点（TPR，FPR）即为ROC图 KS值# 作图步骤： 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）-----这就是截断点依次选取的顺序 按顺序选取截断点，并计算TPR和FPR —也可以只选取n个截断点，分别在1/n，2/n，3/n等位置 横轴为样本的占比百分比（最大100%），纵轴分别为TPR和FPR，计算|TPR-FPR|的ks值，可以得到KS曲线 TPR和FPR曲线分隔最开的位置就是最好的“截断点”，最大间隔距离就是KS值，通常&gt;0.2即可认为模型有比较好的预测准确性 123456789101112131415161718192021222324252627282930313233343536# 模型预测会返回概率，两列，第一列是0的概率，第二列是1的概率proba = lr_model.predict_proba(x)proba# train_thresholds 是阈值，每一个阈值对应roc曲线上的一点train_fpr,train_tpr,train_thresholds = roc_curve(y,proba[:,1])train_ks_arr = abs(train_fpr-train_tpr)train_ks = train_ks_arr.max()print('train KS:',train_ks)# 验证集的ksoft_fpr, oft_tpr,oft_thresholds = roc_curve(oft_y, lr_model.predict_proba(oft_x)[:,1])oft_ks_arr = abs(oft_fpr - oft_tpr)oft_ks = oft_ks_arr.max()print('oft KS:',oft_ks)# 最大值ks对应的下标（画图用）i = train_ks_arr.tolist().index(train_ks)j = oft_ks_arr.tolist().index(oft_ks)import matplotlib.pyplot as pltfrom matplotlib import pyplot as pltplt.plot(train_fpr,train_tpr,label='train roc')plt.plot(train_fpr,abs(train_fpr-train_tpr),label='train ks')plt.scatter(train_fpr[i],abs(train_fpr-train_tpr)[i])plt.plot(oft_fpr, oft_tpr,label='out of time roc')plt.plot(oft_fpr, abs(oft_fpr-oft_tpr),label='out of time ks')plt.scatter(oft_fpr[j],abs(oft_fpr-oft_tpr)[j])plt.plot([0,1],[0,1],'p-.')plt.xlabel('FPR')plt.ylabel('TPR')plt.legend(loc='best')plt.title('ROC Curve')plt.show() PSI群体稳定性指标# PSI(Population Stability Index)的定义# 群体稳定性指标PSI(Population Stability Index)是衡量模型的预测值与实际值偏差大小的指标。 PSI用于评估模型在训练集和时间外样本集上的稳定性指标。 给予的假设是：如果模型是稳定和有效的，那么在几个数据集上人群的分布也应该是稳定的 风控行业常用PSI指标衡量模型或者特征的稳定性，同时也是一种模型效果监控的指标。 PSI = sum[（实际占比-预期占比）* ln（实际占比/预期占比）] 计算举例：# 比如训练一个logistic回归模型，预测时候会有个概率输出p。 以dev为基准，dev上的输出设定为p1，将这个概率值从小到大排序后10等分（实际中等频分箱优于等距分箱）。 现在用这个模型去对新的样本（val或oft）进行预测，预测结果叫p2，按p1的区间也划分为10等分。 实际占比就是p2上在各区间的用户占比，预期占比就是p1上各区间的用户占比。【如果模型是有效的，那么根据p1的区间划分出来的人群占总比和p2划分出来的各个区间的人群占总比应该是大体一致的】 意义就是如果模型跟稳定，那么p1和p2上各区间的用户应该是相近的，占比不会变动很大，也就是预测出来的概率不会差距很大。 仔细想想，PSI就像是两个分布直方图，求了差值后再求和！越小说明模型在不同数据集上预测结果趋于一致，越稳定！ 一般认为PSI小于0.1时候模型稳定性很高，一般认为0.2以下还ok。0.1-0.25一般，大于0.25模型稳定性差，建议重做。 分箱每一箱的样本要大致相同，否则若某一箱太少，造成PSI计算时里面的占比会波动，带来不准确 PS：除了按概率值大小等距十等分外，还可以对概率排序后按数量十等分，两种方法计算得到的psi可能有所区别但数值相差不大。 应用： 样本外测试： 针对不同的样本测试一下模型稳定度，比如训练集与测试集，也能看出模型的训练情况，我理解是看出模型的方差情况。 时间外测试： 测试基准日与建模基准日相隔越远，测试样本的风险特征和建模样本的差异可能就越大，因此PSI值通常较高。至此也可以看出模型建的时间太长了，是不是需要重新用新样本建模了。 模型监控： 模型部署上线后，模型的拒绝率越高，线上的KS越低，也就无法体现模型的真实效果，所以常用PSI值监控线上模型与线下模型的差异，从侧面展示模型真实效果与预期效果的偏差。 特征评估： 将PSI上面第一步的十等分逻辑换成特征取值的分布，对特征进行分箱 在val、oft，或者跨时间段计算PSI 可以评估这个特征随着时间的推移，他的分布是否稳定，考虑是否能将特征代入模型。 如下图： 先把左边的概率或者score十等分，然后在actual（基准数据集，如dev）上面将样本进行划分 对上面dev上各个箱内样本计算各个分箱内的用户站占体用户的比值得到Actual列 同样的上面分箱阈值对Expected数据集进行划分，得到其上各个箱内样本数，计算出各个箱内占总比Expected列 两列相减，乘以 相除的对数值得到index列 sum所有10箱即可得到总体模型的PSI https://www.cnblogs.com/webRobot/p/9133507.html toad底层的PSI实现# 12345678910111213# test 和base 分别是模型在两个数据集上的概率预测输出toad.metrics.PSI(test, base)# 底层实现是先求test 和base 概率值的value_counts，normalize使得返回的数值被归一化。counts成了一个权重# 这里的test_prop是一个series，index是之前test的概率，value是归一化后的countstest_prop = pd.Series(test).value_counts(normalize = True, dropna = False)base_prop = pd.Series(base).value_counts(normalize = True, dropna = False)# 然后两个series相减，只会index相同的相减，也就是概率相同的相减，别的都是Nan# 相除也一样# 得到一个极其稀疏的series，绝大部分都是nan（因为概率相同的很少）# 最后相乘的时候只会乘这些在test和base上概率相同的部分# 再求和psi = np.sum((test_prop - base_prop) * np.log(test_prop / base_prop)) Gini指数# ·GINI系数:也是用于模型风险区分能力进行评估。(使用较少) GINI统计值衡量坏账户数在好账户数上的的累积分布与随机分布曲线之间的面积，好账户与坏账户分布之间的差异越大，GINI指标越高，表明模型的风险区分能力越强。 GINI系数的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。 https://www.zhihu.com/question/37405102/answer/106668941 捕获率capture rate# 我们是低分拒绝模型，希望低分段内坏人尽量多，好人尽量少误伤！ 计算步骤： （1）将用户分数降序排列 （2）对排序后的用户进行等频分箱 （3）计算【累计到每一箱的累计负样本数】 占 【所有负样本】的比值。（cumsum/sum） 用于衡量在低分区间捕捉坏客户的能力，希望模型能尽可能的在更靠前的箱内，捕捉出更多比例的坏人 分段 2018/12/31 capture 5% 13.5% capture 10% 24.2% capture 20% 48.8% 一些表现比较好的B卡，前20%用户中可以捕获到7 80%的坏人。 减少过拟合方法：# 更多的数据 对变量做筛选（模型更简单） 分箱。因为分箱的时候，减少了过拟合（变量变得更稳定，降低噪音） https://www.jianshu.com/p/c61ae11cc5f6","categories":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/categories/risk/"}],"tags":[{"name":"评分卡","slug":"评分卡","permalink":"https://blog.sofunnyai.com/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"},{"name":"特征工程","slug":"特征工程","permalink":"https://blog.sofunnyai.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}]},{"title":"给hexo的博文添加图床、博文加密","slug":"hexo博客的图床和博文密码解决方案","date":"2018-01-07T07:59:42.000Z","updated":"2020-05-23T08:22:38.607Z","comments":true,"path":"article/hexo-blog-img-password.html","link":"","permalink":"https://blog.sofunnyai.com/article/hexo-blog-img-password.html","excerpt":"","text":"关于博客的图片 关于博文加密 关于博客的图片# 1.少量图片可以丢到根文件的source/images文件夹下，算是可以解决。 2.多一点的图片可以丢到当前文件的同目录同名文件夹下。在_config.yml打开这个注释post_asset_folder: true 就会在hero new xxx的时候自动创建xxx目录放静态资源。（但是费劲，url变化后有问题） hexo新版不支持![img](image_url)的正确渲染了，无法保证路径可以渲染成功。官方推荐用他的标签: 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; 更多见 https://hexo.io/zh-cn/docs/asset-folders.html 但是这种方式不是标准markdown语法，无法在我们的markdown编辑器里面正确显示，真是太low了！ 3.所以我建议使用图床神器：ipic、和picgo 戳这里：https://github.com/Molunerfinn/PicGo 大体原理就是可以一键自动上传图片到github或者gitee图床，妈妈再也不用担心我们的图片了。下面是picgo和typora编辑器配合的配置，爽到爆： 关于博文加密# 个别私有博文不方便暴露，需要给博文添加密码，因为我们没有动态服务器去存储密码，只能是在渲染的时候加密，浏览的时候前台js解密。 经过搜寻找到一个工具叫做hexo-blog-encrypt，在Github这里。它会使用对称加密把博文的内容真正加密成密文，只有用户输入密码正确后才会解密成功。 中文介绍在这里，使用起来也很简单，在hexo的主目录安装加密插件： 1cnpm install --save hexo-blog-encrypt 安装完插件后，在hexo的主目录配置一下这个插件_config.yml，添加加密的安全配置： 1234567# Securityencrypt: # hexo-blog-encrypt abstract: 本文为加密的内容, 请输入密码后查看。 message: Password Here： template: &lt;div id=\"hexo-blog-encrypt\" data-wpm=\"&#123;&#123;hbeWrongPassMessage&#125;&#125;\" data-whm=\"&#123;&#123;hbeWrongHashMessage&#125;&#125;\"&gt;&lt;div class=\"hbe-input-container\"&gt;&lt;input type=\"password\" id=\"hbePass\" placeholder=\"&#123;&#123;hbeMessage&#125;&#125;\" /&gt;&lt;label&gt;&#123;&#123;hbeMessage&#125;&#125;&lt;/label&gt;&lt;div class=\"bottom-line\"&gt;&lt;/div&gt;&lt;/div&gt;&lt;script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"&#123;&#123;hbeHmacDigest&#125;&#125;\"&gt;&#123;&#123;hbeEncryptedData&#125;&#125;&lt;/script&gt;&lt;/div&gt; wrong_pass_message: wrong password, try again! wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容. 然后编辑一下博文的模板文件，把密码字段加到头上： 1vim scaffolds/post.md 就像下面这样，password框里如果是空的就不会加密，否则就会加密： 123456789101112131415---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:- tag1- tag2categories:- xxurlname: 修改我xxxx.htmlpassword:---&lt;!--此处生成目录--&gt;&lt;!-- toc --&gt;&lt;!--下面是latex渲染框架katex样式所需的css，不使用latex的话可以删掉--&gt;&lt;link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\"&gt; 这样在列表的时候摘要会显示上面的abstract中的内容，输入框提示message消息。","categories":[{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/categories/hexo/"}],"tags":[{"name":"闲杂","slug":"闲杂","permalink":"https://blog.sofunnyai.com/tags/%E9%97%B2%E6%9D%82/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/tags/hexo/"}]},{"title":"Hexo+Github+自定义域名+CDN搭建博客系统（附namesilo优惠码）","slug":"基于hexo和github构建博客","date":"2018-01-06T05:05:26.000Z","updated":"2020-05-23T08:26:59.245Z","comments":true,"path":"article/build-a-blog-base-on-hexo-github.html","link":"","permalink":"https://blog.sofunnyai.com/article/build-a-blog-base-on-hexo-github.html","excerpt":"","text":"Why What How Details 安装Hexo 选择主题 gayhub托管 自定义域名[可选] Others 生成目录 评论管理 关于latex公式显示 在blog根目录给hexo安装hexo-renderer-markdown-it-plus插件 CDN加速[可选] FAQ Why# 之前分别在csdn和cnblogs写了一些文章，csdn的广告越来越过分实在忍不了。然而cnblogs的markdown编辑器又太弱，一直也没什么更新，所以就再造一个轮子。 What# Hexo是个啥，这一套是怎么工作的？ Hexo是一个基于Nodejs的渲染引擎，可以集成多个主题和插件，实现了内容和样式分离，可以根据喜好快速换装。 用户可以撰写一个markdown格式的博客文件，使用Hexo渲染为html格式 然后将html部署到github（或者自有服务器/vps等等） 使用github的pages服务（或者自有服务器的IP）即可访问我们的博客 可选：接着可以用我们的自有域名解析到github的pages服务即可（或者我们服务器的IP） 搭建好了怎么写博客？ 本地写markdown格式，安利下typora，巨好用。 写完执行一个命令会自动渲染成html，再执行一个命令会自动部署到github，相当简单。 How# 把大象关进冰箱需要三步，搭建基于Hexo的博客也需要三步： 安装Hexo，跑起来 搞一个Gayhub的repo，弄一个page.io 搞一个域名，DNS解析即可（如果需要加速，在CDN配置一下） Details# 安装Hexo# 首选需要安装nodejs，安装cnpm 选择操作系统的发行版: https://nodejs.org/en/download/ 我是linux，下载解压，配置环境变量，source一把即可。 windows用户更简单，下载后各种next即可。 mac和linux类似 npm加速，安装cnpm： npm install -g cnpm --registry=https://registry.npm.taobao.org 安装Hexo：cnpm install -g hexo 会自动从gayhub下载hexo并安装 然后就进入目录去配置_conifg.yml,包括博客title、作者、语言等等 运行 hexo s，然后去http://localhost:4000就能看到了。默认样式会有点丑，别着急看下一节。 选择主题# 先戳这里，官方有巨多主题： https://hexo.io/themes/ 也可以去gayhub自己搜hexo-theme即可，也有n多 我一个老年大叔，选择了一个简单一点的主题hexo-theme-pure，这个https://github.com/cofess/hexo-theme-pure 主题的中文说明：https://github.com/cofess/hexo-theme-pure/blob/master/README.cn.md 按照主题说明，clone到hexo的theme文件夹内，然后修改一下hexo的_config.yml文件中的主题theme: pure 按照主题说明，安装主题渲染所需的nodejs插件。无非就是几个cnpm install xxxx即可 按照主题说明，配置主题的配置文件，一般在主题文件夹./hexo/theme/pure下的_config.yml（无非是颜色、元素是否显示、布局之类的），很简单看一眼就知道。 运行渲染hexo clean &amp;&amp; hexo g &amp;&amp; hexo s，分别是清理、生成、运行，然后再去http://localhost:4000看一眼，主题就生效了。 gayhub托管# 我们hexo g渲染生成的静态文件在public文件夹内，需要把它丢到一个web容器内运行就可以了。gayhub提供githubpages服务可以托管静态文件，并可以http浏览。 所以去github新建一个repo，repo名字为xxxx.github.io,这个xxxx必须和你的github用户名一致! 然后回到hexo中配置deploy模式为git，配置仓库地址为上面的repo地址。更多参见https://hexo.io/zh-cn/docs/github-pages 配置完毕hexo d输入github账号密码即可push到服务器（如果本机没有保存，或者服务器配置秘钥的话，具体github配置公钥上网查找） 然后可以访问我们的博客了https://xxxx.github.io` 自定义域名[可选]# 上面虽然博客可以访问了，但是github.io看起来有点low，而且国内访问速度也很慢。 所以，我建议撸一个域名，挂博客，搞微信开发，内网穿透，扶墙等等用处多多。。。而且最好是境外服务商域名，境内的域名要备案、年检，非常非常麻烦。 目前最便宜的是戳这个：namesilo官网，优势： 他家的.com域名只要7.99刀，.xyy和.online域名只要0.99刀，简直白送！关键是续费便宜没有坑，别家有首年很便宜，后面续费巨贵的。 永久免费的whois隐私保护，其他家这个功能还要收费。 支持支付宝收款，不用别家还要信用卡或者PayPal 所以戳链接进去官网： www.namesilo.com/ 进入官网后，右上角注册sign up，输入用户名，邮箱，密码即可。 然后register，选择域名进行注册，第一次会让你填写信息（以免域名丢失找回，或者服务商后续通知一些域名相关事项） 输入你想注册的域名，搜索，看看有没有被注册过： 假如你选择的没有被占用。点击下面的add加入购物车，然后checkout结算,我这里用.com的8.99刀域名举例（你也可以用下面0.99刀的，简直便宜到几乎白送！） 结算页面，按图上选择即可，然后下面的打折码输入**cutoff** ，点击submit即可享受打折优惠！ 下一步就是支付宝扫码付款即可。 付完款去account个人中心，点击domain manager域名管理，会出来你的域名列表。点击设置dns 设置DNS,点击CNAME，会出来一条解析，可以根据喜好设置为www的主域名，还是blog.xxx.com的二级域名。目标设置我们上面个人的xxxxxx.github.io，提交即可。 搞定你以为大功告成了？还需要在github的repo里面设置这个域名，否则github会阻止域名解析，导致404。看下一节 github配置域名： 在那个custom domain填写你的域名，save一下 然后在你本地的hexo/source目录下创建个CNAME文件夹，写上你的域名 最后，重新hexo g &amp; hexo d 此时即可用我们的域名访问。 Others# 生成目录# 为博客生成toc目录 使用插件cnpm install hexo-toc --save 然后配置一下最大深度等 123456789toc: maxdepth: 3 class: toc slugify: transliteration decodeEntities: false anchor: position: after symbol: '#' style: header-anchor 评论管理# 关于博客的评论，一般来说是需要一个数据库的。但是我们是纯静态服务，所以有人搞了类似gitalk、gitment 这样的东西。只用前台引用js即可，一般都不用我们管，主题已经集成好了。只用按照说明进行配置几个参数即可。 关于latex公式显示# latex会显示失败，按照pure主题的解决方案： 数学公式# Hexo默认使用&quot;hexo-renderer-marked&quot;引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签 解决方案# 解决方案有很多，可以网上搜下，为了节省大家的时间，这里只提供亲身测试过的方法。 更换Hexo的markdown渲染引擎，hexo-renderer-markdown-it-plus引擎替换默认的渲染引擎hexo-renderer-marked即可。 在blog根目录给hexo安装hexo-renderer-markdown-it-plus插件# 123cnpm un hexo-renderer-marked --save # 卸载cnpm i hexo-renderer-markdown-it-plus --save # 安装新的渲染框架cnpm i markdown-it-katex --save # 安装katex渲染latex 配置# 安装插件后，如果未正常渲染LaTeX数学公式，在博客根目录配置文件_config.yml中添加 12345678910111213141516markdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: “”‘’ plugins: - plugin: name: markdown-it-katex enable: true - plugin: name: markdown-it-mark enable: false 文章启用mathjax（不用设置true也可以）# 12title: Hello Worldmathjax: true 按照上面操作了还是不行，元素错位。看了下面这个解决了： 还错位或者显示元素不准的话，需要引入一个katex的css# 1&lt;link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\"&gt; https://blog.csdn.net/u014792304/article/details/78687859 katex和latex大部分命令是相通的，不同的略微差别见下面： https://khan.github.io/KaTeX/function-support.html CDN加速[可选]# 想继续折腾访问速度的往下看： 首先，不推荐国内的CDN供应商和域名解析商，因为他们动不动就有合规要求。会折腾客户去备案（不是所有的） 然后，推荐cloudflare，也很简单，注册一个账号，在namesilo里面把CDN服务迁移到cloudflare即可。上网搜索巨多资料。 FAQ# 为啥配置好了域名无法访问？ 等待域名解析，尝试DNS那里的TTL设置小一点，刷新本地DNS缓存。 为啥速度不一般？ 因为是国外DNS，可能第一次访问有点慢。有精力的尝试迁移到cloudflare，还有CDN加速，https，爽歪歪。戳这里：https://www.cloudflare.com/","categories":[{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/categories/hexo/"}],"tags":[{"name":"闲杂","slug":"闲杂","permalink":"https://blog.sofunnyai.com/tags/%E9%97%B2%E6%9D%82/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/tags/hexo/"}]},{"title":"Java Hashmap底层源码剖析","slug":"hashmap","date":"2017-03-19T05:09:19.000Z","updated":"2020-05-30T16:21:43.450Z","comments":true,"path":"article/java-hashmap.html","link":"","permalink":"https://blog.sofunnyai.com/article/java-hashmap.html","excerpt":"","text":"Hash数据结构基础 哈希表： 哈希表的缺点： Hash碰撞解决？—开放地址/链地址 好的哈希函数: Hash数据结构基础# 哈希表：# 一个基于数组的K-V存储结构，K经过hash函数后散列到数组的某个位置，去拿到所需的K-V对。适用于可预测数据量且不需要遍历的场合。 哈希表的效率：插入和删除需要O(1)的时间。(既能够具备数组的快速查询的优点又能融合链表方便快捷的增加删除元素的优势) 哈希表的缺点：# 基于数组，由于数组是定长的，长度塞满时效率很低（碰撞太多）。 所以程序需要在合适的时候（如loadFactor大于2/3）把数据移动到更大的哈希表中，这个操作非常费时（因为地址是和数组长度有关的，移动时所有的元素将会逐一重新算hash地址）。 没有一种机制可以有序遍历(如从小到大)哈希表中的数据，如果需要则只能借助使用其他的数据结构。 Hash碰撞解决？—开放地址/链地址# 多个对象通过hash算法后的的地址相同，就产生了“哈希碰撞”。解决哈希碰撞的方法一般有两种： 开放地址法：即在数组中向后找一个合适的空位存储新的碰撞的对象。 线性探测：若碰撞，则从第一次hash的地址开始逐一向后找，找到空位则存储新的对象。（这种算法会在数组的某一段聚集大量的值，会增大碰撞的概率。聚集会降低hash表的效率） 二次探测：若碰撞，则从第一次hash的地址开始继续向后找，但每次找的步长为 1、4、9、16、25…以平方数递增。（比上一种略好，但因为碰撞元素探测的补偿相同，故会造成更细小的碰撞，在数组上聚集很多小段连续元素） 再hash方法：通过另一个hash算法算出第二次的步长。这个步长为一个与hash地址有关的值。如：stepSize = constant - （key % constant） key为第一次hash后碰撞的地址，这个常量要求是小于数组长度的质素。（数组长度也是质数） 链地址法：即在发生碰撞的位置存储一个链表。 好的哈希函数:# 可以快速的计算,有许多乘法/除法会比较慢不合适,除了取模之外可以考虑位运算,左移N位相当于除以2的N次方 结果要随机,哈希函数为了避免哈希碰撞,需要把key映射到数组的下标,越随机越好 使用质数取模,也就是数组长度要是质数,否则会有很大的聚集效应! 字符串hash的时候可以把每一位按照ascii码乘以27的N次方,再相加. 如china的c计算: (charAt(0)-97)*27^4 子母h:(charAt(1)-97)*27^3 … 如果最后数字太大可以使用[同余定理]拆分: (A×B + C×D + E×F +...)%P == ((A×B)%P + (C×D)%P +...)%P 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165package com.sam.phoenix.datastructure.e_HashTable;import java.math.BigDecimal;/*** 本例子采用线性线性探测法 * @author sam */public class HashTableLine &#123; //最大可以存放数据的总数 private int maxDataSize = 0; //数组长度是arraySize除以loadFactor,除完的结果最好是一个质数,否则会有比较大的聚集效应 private Node[]hashArray; //已经使用的长度 private int length = 0; //删除标记 static final Node DELETEED_NODE = new Node(Integer.MIN_VALUE,null); /** * 构造方法，构造数组，初始化数组长度 * * 这里初始化一个比最大长度更长的数组,解决哈希碰撞开放地址后效率下降的问题,用空间换取时间 * @param size 最大存储的数量 * @param loadFactor */ public HashTableLine(int size, float loadFactor)&#123; if(loadFactor &lt;= 0 || loadFactor&gt;1)&#123; System.out.println(\"装载因子必须大于0,小于1,因为输入非法,默认取0.5!!!!\"); loadFactor = 0.5f; &#125; int len = new BigDecimal(size).divide(new BigDecimal(loadFactor),0,BigDecimal.ROUND_HALF_EVEN).intValueExact(); hashArray = new Node[len]; maxDataSize = size; &#125; /** * 哈希算法，根据key的大小，算出在数组中的位置 * @param key * @return */ private int hashFunction(int key)&#123; return key % hashArray.length; &#125; /** * 向hash表插入key-value数据,采用线性探测,如果插入的位置已经有值,自动往后面寻找一个空的位置存储 * @param key * @param value */ public void insert(int key, Object value)&#123; if(length == maxDataSize)&#123; System.out.println(\"已经满了,无法插入!!!!!!\"); //再插入就超过装载因子,碰撞太多,性能急剧下降 &#125; int index = hashFunction(key); //该位置恰好空余,可以插入.发生哈希碰撞,采用线性探测往后找, while(index &lt; hashArray.length)&#123; if(hashArray[index] == null || hashArray[index] == DELETEED_NODE)&#123; //插 hashArray[index] = new Node(key,value); //记录长度 length++; return; &#125; //此处使用线性探测: 每次+1,线性往后探测空余位置 //[二次探测:不使用index++,而是使用每次往后1,4,9...平方探测,将连续的大区域碰撞转化为分散的碎片碰撞导致二次聚集(因为步长是确定的)] //[再hash: 将当前的index再次使用一个不同的hash算法计算后往后找,直到找到目标或者发现null,可以消除原始聚集和二次聚集(步长是不确定的)] index++; //找到了数组最后面,任然没有空余位置 if(index == hashArray.length-1)&#123; //返回数组最前面继续找 index = 0; &#125; &#125; &#125; /** * 查找对象 * @param key * @return */ public Object find(int key)&#123; //是否为空 if(length == 0)&#123; return null; &#125; int index = hashFunction(key); //往后找 while(index &lt; hashArray.length)&#123; //找到一个空节点还没有,说明没有该条记录 if(hashArray[index] == null)&#123; return null; &#125; //该节点有数据,判断是不是目标节点,不是的话可能是碰撞过的,往后找 if(hashArray[index].getKey() == key)&#123; return hashArray[index].getValue(); &#125;else&#123; index++; &#125; if(index == hashArray.length-1)&#123; //找到最后还没有,从队首开始找 index = 0; &#125; &#125; //所有节点都非空,找了还没有 return null; &#125; /** * 从hash表删除 * @param key * @return */ public boolean delete(int key)&#123; //不能硬删除!!!!! 否则之前因为哈希碰撞线性探测存储的数据就找不到了!!!!!!! int index = hashFunction(key); if(hashArray[index]==null)&#123; return false; &#125; //往后找,找到这个元素就软删除,下次查询到这里直接跳过往后找... while(index &lt; hashArray.length)&#123; //找到一个空节点还没有,说明没有该条记录 if(hashArray[index] == null)&#123; return false; &#125; //该节点有数据,判断是不是目标节点,不是的话可能是碰撞过的,往后继续找 if(hashArray[index].getKey() == key)&#123; //找到了，软删除之 hashArray[index] = DELETEED_NODE; length--; return true; &#125;else&#123; index++; &#125; if(index == hashArray.length-1)&#123; //找到最后还没有,从队首开始找 index = 0; &#125; &#125; //没找到 return false; &#125; /** * 打印hash表 */ public void display()&#123; for(int i = 0; i &lt; hashArray.length; i ++)&#123; if(hashArray[i]==null)&#123; System.out.print(\"null,\"); continue; &#125; System.out.print(hashArray[i].getKey()+\"\"+hashArray[i].getValue()+\",\"); &#125; System.out.println(); &#125; /** * 获取长度 */ public int getSize() &#123; return this.maxDataSize; &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://blog.sofunnyai.com/categories/java/"}],"tags":[{"name":"源码","slug":"源码","permalink":"https://blog.sofunnyai.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"java","slug":"java","permalink":"https://blog.sofunnyai.com/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.sofunnyai.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}],"categories":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/categories/risk/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://blog.sofunnyai.com/categories/SpringCloud/"},{"name":"network","slug":"network","permalink":"https://blog.sofunnyai.com/categories/network/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.sofunnyai.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"IO","slug":"IO","permalink":"https://blog.sofunnyai.com/categories/IO/"},{"name":"多线程","slug":"多线程","permalink":"https://blog.sofunnyai.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/categories/hexo/"},{"name":"java","slug":"java","permalink":"https://blog.sofunnyai.com/categories/java/"}],"tags":[{"name":"risk","slug":"risk","permalink":"https://blog.sofunnyai.com/tags/risk/"},{"name":"strategy","slug":"strategy","permalink":"https://blog.sofunnyai.com/tags/strategy/"},{"name":"评分卡","slug":"评分卡","permalink":"https://blog.sofunnyai.com/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://blog.sofunnyai.com/tags/SpringCloud/"},{"name":"网络","slug":"网络","permalink":"https://blog.sofunnyai.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.sofunnyai.com/tags/hexo/"},{"name":"运维","slug":"运维","permalink":"https://blog.sofunnyai.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.sofunnyai.com/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.sofunnyai.com/tags/mysql/"},{"name":"基础","slug":"基础","permalink":"https://blog.sofunnyai.com/tags/%E5%9F%BA%E7%A1%80/"},{"name":"jdk","slug":"jdk","permalink":"https://blog.sofunnyai.com/tags/jdk/"},{"name":"源码","slug":"源码","permalink":"https://blog.sofunnyai.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"多线程","slug":"多线程","permalink":"https://blog.sofunnyai.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"并发","slug":"并发","permalink":"https://blog.sofunnyai.com/tags/%E5%B9%B6%E5%8F%91/"},{"name":"特征工程","slug":"特征工程","permalink":"https://blog.sofunnyai.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"闲杂","slug":"闲杂","permalink":"https://blog.sofunnyai.com/tags/%E9%97%B2%E6%9D%82/"},{"name":"java","slug":"java","permalink":"https://blog.sofunnyai.com/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.sofunnyai.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}